Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Title Page i,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,1,1,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555603,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Title Page iii,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,2,2,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555879,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Table of Contents,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,4,15,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555689,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
"Message from the MSR 2024 General, Program, and Junior PC Chairs",,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,16,19,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555795,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Message from the MSR 2024 Data and Tool Showcase Track Co-Chairs,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,20,20,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555776,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Message from the MSR 2024 Industry Track Co-Chairs,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,21,21,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555705,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Message from the MSR 2024 Mining Challenge Co-Chairs,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,22,24,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555856,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Message from the MSR 2024 Registered Reports Track Co-Chairs,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,25,26,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555837,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Message from the MSR 2024 Tutorials Track Co-Chairs,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,27,27,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555624,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Message from the MSR 2024 Vision and Reflection Track Co-Chairs,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,28,28,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555596,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Organizing Committee,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,29,30,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555665,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Program Committee,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,31,39,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555874,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
Thirty-Three Years of Mathematicians and Software Engineers: A Case Study of Domain Expertise and Participation in Proof Assistant Ecosystems,G. Lincroft; M. Cho; K. Hough; M. Bazzaz; J. Bell,"Northeastern University, Boston, Massachusetts, USA; Northeastern University, Boston, Massachusetts, USA; Northeastern University, Boston, Massachusetts, USA; Northeastern University, Boston, Massachusetts, USA; Northeastern University, Boston, Massachusetts, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,1,13,"As technical computing software, such as MATLAB and SciPy, has gained popularity, ecosystems of interdependent software solutions and communities have formed around these technologies. The development and maintenance of these technical computing ecosystems requires expertise in both software engineering and the underlying technical domain. The inherently interdisciplinary nature of these ecosystems presents unique challenges and opportunities that shape software development practices.Proof assistants, a type of technical computing software, aid users in the creation of formal proofs. In order to examine the influence of the underlying technical domain — mathematics — on the development of proof assistant ecosystems, we mined participant activity data from the code repositories and social channels of three popular proof assistants: Lean, Coq, Isabelle. Despite having a shared technical domain, we found little cross-pollination between contributors to the proof assistants. Additionally, we found that most long-term developers focused solely on technical work and did not participate in official social channels. We also found that proof assistant developers specialized into technical subfields. However, the proportion of specialists varied between ecosystems. We did not find evidence that these specialties contributed to fractures within the ecosystems. We discuss the implications of these results on the long-term health and sustainability of proof assistant ecosystems.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644908,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555745,,Surveys;Shape;Ecosystems;Software;Mathematics;Data mining;Sustainable development,,,,90.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Boosting API Misuse Detection via Integrating API Constraints from Multiple Sources,C. Li; J. Zhang; Y. Tang; Z. Li; T. Sun,"College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,14,26,"In modern software development, developers access reusable functionality provided by third-party libraries through Application Programming Interfaces (APIs). However, using APIs requires developers to conform specific constraints and guidelines, otherwise it may lead to API misuses. Existing approaches for API misuse detection often rely on analyzing API documentation or mining client code. However, these approaches are limited by the quality of API documentation and the naive assumption that deviations from common usage patterns in client code imply potential API misuses, making them less reliable. In this paper, we propose an approach that comprehensively integrates the API usage constraints from multiple sources, including client code, API documentation, and library code, to detect API misuses. First, we convert client code into API Usage Graphs (AUGs), extract the API usage patterns, and apply heuristic filtering rules to obtain API usage constraints. Meanwhile, we also analyze library code and API documentation to obtain various API usage constraints. Next, we combine the obtained API usage constraints from multiple sources together to generate a series of API preliminary constraint graphs. Based on these API preliminary constraint graphs, we design constraint alternative strategies to form API alternative constraint graphs. Finally, we parse the Abstract Syntax Tree (AST) of the test code and match it against API (alternative) constraint graphs to detect API misuses. The experimental evaluation demonstrates that our approach achieves the Precision of 72.22% and the Recall of 43.01% on the MUBench dataset, with an F1 score of 53.91%. These values significantly out-perform existing state-of-the-art API misuse detection approaches, highlighting the effectiveness of integrating API usage constraints from various sources. Additionally, the designed heuristic filtering rules and constraint alternative strategies significantly reduce false positives, enhancing the Precision of our approach in API misuse detection.CCS CONCEPTS• Software and its engineering → Software libraries and repositories; Software maintenance tools.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644904,Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555695,API Misuse Detection;API Constraint Extraction;API Usage Graphs;API Constraint Graphs,Software maintenance;Codes;Software libraries;Filtering;Documentation;Syntactics;Data mining,,,,57.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Availability and Usage of Platform-Specific APIs: A First Empirical Study,R. Job; A. Hora,"IFPB, Cajazeiras, Brazil; Department of Computer Science, UFMG, Belo Horizonte, Brazil",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,27,31,"A platform-specific API is an API implemented for a particular platform (e.g., operating system), therefore, it may not work on other platforms than the target one. In this paper, we propose a first empirical study to assess the availability and usage of platform-specific APIs. We analyze the platform-specific APIs provided by the Python Standard Library and mine their usage in 100 popular systems. We find that 21% of the Python Standard Library APIs are platform-specific and that 15% of the modules contain at least one. The platforms with the most availability restrictions are WASI (43.69%), Emscripten (43.64%), Unix (6.76%), and Windows (2.12%). Moreover, we find that platform-specific APIs are largely used in Python. We detect over 19K API usages in all 100 projects, in both production (52.6%) and test code (47.4%). We conclude by discussing practical implications for practitioners and researchers.CCS CONCEPTS• Software and its engineering → Software testing and debugging.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644925,EMI; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555586,software testing;mining software repositories;test smells;Python,Software testing;Codes;Operating systems;Production;Debugging;Software;Libraries,,,,25.0,,18 Jun 2024,,,IEEE,IEEE Conferences
AndroLibZoo: A Reliable Dataset of Libraries Based on Software Dependency Analysis,J. Samhi; T. F. Bissyandé; J. Klein,"CISPA – Helmholtz Center for Information Security; SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,32,36,"Android app developers extensively employ code reuse, integrating many third-party libraries into their apps. While such integration is practical for developers, it can be challenging for static analyzers to achieve scalability and precision when libraries account for a large part of the code. As a direct consequence, it is common practice in the literature to consider developer code only during static analysis –with the assumption that the sought issues are in developer code rather than the libraries. However, analysts need to distinguish between library and developer code. Currently, many static analyses rely on white lists of libraries. However, these white lists are unreliable, inaccurate, and largely non-comprehensive.In this paper, we propose a new approach to address the lack of comprehensive and automated solutions for the production of accurate and ""always up to date"" sets of libraries. First, we demonstrate the continued need for a white list of libraries. Second, we propose an automated approach to produce an accurate and up-to-date set of third-party libraries in the form of a dataset called AndroLibZoo. Our dataset, which we make available to the community, contains to date 34 813 libraries and is meant to evolve.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644866,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555749,Android Static Analysis;Android Libraries,Codes;Accesslists;Accuracy;Scalability;Static analysis;Production;Libraries,,,,30.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Goblin: A Framework for Enriching and Querying the Maven Central Dependency Graph,D. Jaime; J. E. Haddad; P. Poizat,"CNRS, LIP6, Sorbonne Université, Paris, France; CNRS, LAMSADE, Université Paris Dauphine-PSL, Paris, France; CNRS, LIP6, Sorbonne Université, Paris, France",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,37,41,"Dependency graphs support software maintenance and software ecosystem analysis. Several metrics can be used on top of these graph models but the set of such metrics is to evolve over time. Further, some metrics have a dynamic nature, requiring being able to ""rewind"" dependency graphs at some point in time. To address these issues we propose the Goblin framework. It is composed of a dependency graph metamodel with time-related information, a miner to retrieve the graph from Maven Central, and a tool for on-demand metric weaving into dependency graphs. As a whole, Goblin is a customizable framework for ecosystem and dependency analysis. This is illustrated with a set of complementary experiments. Our tools, datasets, and experiments are freely available online.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644879,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555727,software ecosystem;dependency graph;framework;dataset;mining software repositories;maven central,Measurement;Software maintenance;Costs;Biological system modeling;Ecosystems;Rhythm;Libraries,,6.0,,11.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Dataset: Copy-based Reuse in Open Source Software,M. Jahanshahi; A. Mockus,"University of Tennessee, Knoxville, USA; University of Tennessee Vilnius University, Knoxville, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,42,47,"In Open Source Software, the source code and any other resources available in a project can be viewed or reused by anyone subject to often permissive licensing restrictions. In contrast to some studies of dependency-based reuse supported via package managers, no studies of OSS-wide copy-based reuse exist. This dataset seeks to encourage the studies of OSS-wide copy-based reuse by providing copying activity data that captures whole-file reuse in nearly all OSS. To accomplish that, we develop approaches to detect copybased reuse by developing an efficient algorithm that exploits World of Code infrastructure: a curated and cross referenced collection of nearly all open source repositories. We expect this data will enable future research and tool development that support such reuse and minimize associated risks.CCS CONCEPTS•Software and its engineering → Software creation and management; • General and reference → Empirical studies.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644868,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555700,Reuse;Open Source Software;Software Development;Copy-based Reuse;Software Supply Chain;World of Code,Codes;Source coding;Software algorithms;Data mining;Open source software,,1.0,,45.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Mining Our Way Back to Incremental Builds for DevOps Pipelines,S. McIntosh,"Software REBELs, University of Waterloo, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,48,49,"The incremental build is a key feature of build automation tools. It still plays a key role in the build systems that underpin DevOps pipelines. Yet it is quite common for these ""upper layer"" automation technologies to start from a clean copy of the codebase, rendering the incremental builds inert. In this tutorial, we discuss why it is desirable to restore the incremental build features of the past. We also describe past and ongoing work that strives to make DevOps pipelines operate incrementally again. Finally, we discuss perceived barriers to adoption that our past solutions have faced.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3649106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555815,Mining Software Repositories;DevOps Pipelines;Incremental Builds,DevOps;Automation;Pipelines;Tutorials;Rendering (computer graphics);Software;Data mining,,,,18.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Enhancing Performance Bug Prediction Using Performance Code Metrics,G. Zhao; S. Georgiou; Y. Zou; S. Hassan; D. Truong; T. Corbin,"IBM, Markham, Canada; simpleTechs, Budapest, Hungary; Department of Electrical and Computer Engineering, Queen’s University, Kingston, Canada; Faculty of Information, University of Toronto, Toronto, Canada; IBM, Markham, Canada; IBM, Southampton, United Kingdom",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,50,62,"Performance bugs are non-functional defects that can significantly reduce the performance of an application (e.g., software hanging or freezing) and lead to poor user experience. Prior studies found that each type of performance bugs follows a unique code-based performance anti-pattern and proposed different approaches to detect such anti-patterns by analyzing the source code of a program. However, each approach can only recognize one performance anti-pattern. Different approaches need to be applied separately to identify different performance anti-patterns. To predict a large variety of performance bug types using a unified approach, we propose an approach that predicts performance bugs by leveraging various historical data (e.g., source code and code change history). We collect performance bugs from 80 popular Java projects. Next, we propose performance code metrics to capture the code characteristics of performance bugs. We build performance bug predictors using machine learning models, such as Random Forest, eXtreme Gradient Boosting, and Linear Regressions. We observe that: (1) Random Forest and eXtreme Gradient Boosting are the best algorithms for predicting performance bugs at a file level with a median of 0.84 AUC, 0.21 PR-AUC, and 0.38 MCC; (2) The proposed performance code metrics have the most significant impact on the performance of our models compared to code and process metrics. In particular, the median AUC, PR-AUC, and MCC of the studied machine learning models drop by 7.7%, 25.4%, and 20.2% without using the proposed performance code metrics; and (3) Our approach can predict additional performance bugs that are not covered by the anti-patterns proposed in the prior studies.CCS CONCEPTS•Software and its engineering Software testing and debugging; Software testing and debugging",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644920,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555738,Performance bugs;Performance anti-patterns;Performance code metrics;Performance bug prediction,Measurement;Software testing;Java;Codes;Machine learning algorithms;Source coding;Computer bugs,,1.0,,99.0,,18 Jun 2024,,,IEEE,IEEE Conferences
An Investigation of Patch Porting Practices of the Linux Kernel Ecosystem,X. Li; Z. Zhang; Z. Qian; T. Jaeger; C. Song,UC Riverside; UC Riverside; UC Riverside; UC Riverside; UC Riverside,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,63,74,"Open-source software is increasingly reused, complicating the process of patching to repair bugs. In the case of Linux, a distinct ecosystem has formed, with Linux mainline serving as the upstream, stable or long-term-support (LTS) systems forked from mainline, and Linux distributions, such as Ubuntu and Android, as downstreams forked from stable or LTS systems for end-user use. Ideally, when a patch is committed in the Linux upstream, it should not introduce new bugs and be ported to all the applicable downstream branches in a timely fashion. However, several concerns have been expressed in prior work about the responsiveness of patch porting in this Linux ecosystem. In this paper, we mine the software repositories to investigate a range of Linux distributions in combination with Linux stable and LTS, and find diverse patch porting strategies and competence levels that help explain the phenomenon. Furthermore, we show concretely using three metrics, i.e., patch delay, patch rate, and bug inheritance ratio, that different porting strategies have different tradeoffs. We find that hinting tags(e.g., Cc stable tags and fixes tags) are significantly important to the prompt patch porting, but it is noteworthy that a substantial portion of patches remain devoid of these indicative tags. Finally, we offer recommendations based on our analysis of the general patch flow, e.g., interactions among various stakeholders in the ecosystem and automatic generation of hinting tags, as well as tailored suggestions for specific porting strategies.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644902,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555642,,Measurement;Linux;Ecosystems;Computer bugs;Maintenance engineering;Delays;Stakeholders,,,,63.0,,18 Jun 2024,,,IEEE,IEEE Conferences
CrashJS: A NodeJS Benchmark for Automated Crash Reproduction,P. Oliver; J. Dietrich; C. Anslow; M. Homer,"Victoria University of Wellington, Wellington, New Zealand; Victoria University of Wellington, Wellington, New Zealand; Victoria University of Wellington, Wellington, New Zealand; Victoria University of Wellington, Wellington, New Zealand",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,75,87,"Software bugs often lead to software crashes, which cost US companies upwards of $2.08 trillion annually. Automated Crash Reproduction (ACR) aims to generate unit tests that successfully reproduce a crash. The goal of ACR is to aid developers with debugging, providing them with another tool to locate where a bug is in a program. The main approach ACR currently takes is to replicate a stack trace from an error thrown within a program. Currently, ACR has been developed for C, Java, and Python, but there are no tools targeting JavaScript programs. To aid the development of JavaScript ACR tools, we propose CrashJS: a benchmark dataset of 453 Node.js crashes from several sources. CrashJS includes a mix of real-world and synthesised tests, multiple projects, and different levels of complexity for both crashes and target programs.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644912,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555775,Automated Crash Reproduction;Benchmark;Data Collection;Dataset;Software Testing;Test Generation,Java;Target tracking;Computer bugs;Benchmark testing;Computer crashes;Software;Complexity theory,,,,43.0,,18 Jun 2024,,,IEEE,IEEE Conferences
An Empirical Study on Just-in-time Conformal Defect Prediction,X. Shahini; A. Metzger; K. Pohl,"Paluno, University of Duisburg-Essen, Essen, Germany; Paluno, University of Duisburg-Essen, Essen, Germany; Paluno, University of Duisburg-Essen, Essen, Germany",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,88,99,"Code changes can introduce defects that affect software quality and reliability. Just-in-time (JIT) defect prediction techniques provide feedback at check-in time on whether a code change is likely to contain defects. This immediate feedback allows practitioners to make timely decisions regarding potential defects. However, a prediction model may deliver false predictions, that may negatively affect practitioners’ decisions. False positive predictions lead to unnecessarily spending resources on investigating clean code changes, while false negative predictions may result in overlooking defective changes. Knowing how uncertain a defect prediction is, would help practitioners to avoid wrong decisions. Previous research in defect prediction explored different approaches to quantify prediction uncertainty for supporting decision-making activities. However, these approaches only offer a heuristic quantification of uncertainty and do not provide guarantees.In this study, we use conformal prediction (CP) as a rigorous uncertainty quantification approach on top of JIT defect predictors. We assess how often CP can provide guarantees for JIT defect predictions. We also assess how many false JIT defect predictions CP can filter out. We experiment with two state-of-the-art JIT defect prediction techniques (DeepJIT and CC2Vec) and two widely used datasets (Qt and OpenStack).Our experiments show that CP can ensure correctness with a 95% probability, for only 27% (for DeepJIT) and 9% (for CC2Vec) of the JIT defect predictions. Additionally, our experiments indicate that CP might be a valuable technique for filtering out the false predictions of JIT defect predictors. CP can filter out up to 100% of false negative predictions and 90% of false positives generated by CC2Vec, and up to 86% of false negative predictions and 83% of false positives generated by DeepJIT.CCS CONCEPTS• Software and its engineering → Software testing and debugging; Formal methods; Software maintenance tools.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644928,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555854,Defect prediction;quality assurance;conformal prediction;machine learning;deep learning;correctness guarantees;uncertainty,Software testing;Software maintenance;Uncertainty;Codes;Filtering;Instruments;Measurement uncertainty,,,,69.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Fine-Grained Just-In-Time Defect Prediction at the Block Level in Infrastructure-as-Code (IaC),M. Begoug; M. Chouchen; A. Ouni; E. A. AlOmar; M. W. Mkaouer,"ETS Montreal, University of Quebec; ETS Montreal, University of Quebec; ETS Montreal, University of Quebec; Stevens Institute of Technology; University of Michigan-Flint",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,100,112,"Infrastructure-as-Code (IaC) is an emerging software engineering practice that leverages source code to facilitate automated configuration of software systems’ infrastructure. IaC files are typically complex, containing hundreds of lines of code and dependencies, making them prone to defects, which can result in breaking online services at scale. To help developers early identify and fix IaC defects, research efforts have introduced IaC defect prediction models at the file level. However, the granularity of the proposed approaches remains coarse-grained, requiring developers to inspect hundreds of lines of code in a file, while only a small fragment of code is defective. To alleviate this issue, we introduce a machinelearning-based approach to predict IaC defects at a fine-grained level, focusing on IaC blocks, i.e., small code units that encapsulate specific behaviours within an IaC file. We trained various machine learning algorithms based on a mixture of code, process, and change-level metrics. We evaluated our approach on 19 open-source projects that use Terraform, a widely used IaC tool. The results indicated that there is no single algorithm that consistently outperforms the others in 19 projects. Overall, among the six algorithms, we observed that the LightGBM model achieved a higher average of 0.21 in terms of MCC and 0.71 in terms of AUC. Models analysis reveals that the developer’s experience and the relative number of added lines tend to be the most important features. Additionally, we found that blocks belonging to the most frequent types are more prone to defects. Our defect prediction models have also shown sensitivity to concept drift, indicating that IaC practitioners should regularly retrain their models.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644934,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555830,Defect Prediction;Infrastructure-as-Code;IaC;Terraform,Codes;Machine learning algorithms;Sensitivity;Source coding;Software algorithms;Predictive models;Prediction algorithms,,2.0,,106.0,,18 Jun 2024,,,IEEE,IEEE Conferences
TrickyBugs: A Dataset of Corner-case Bugs in Plausible Programs,K. Liu; Y. Han; Y. Liu; J. M. Zhang; Z. Chen; F. Sarro; G. Huang; Y. Ma,"Peking University, Beijing, China; Peking University, Beijing, China; Peking University, Beijing, China; King’s College London, London, United Kingdom; University College London, London, United Kingdom; University College London, London, United Kingdom; National Key Laboratory of Data Space Technology and System, Peking University, Beijing, China; Peking University, Beijing, China",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,113,117,"We call a program that passes existing tests but still contains bugs as a buggy plausible program. Bugs in such a program can bypass the testing environment and enter the production environment, causing unpredictable consequences. Therefore, discovering and fixing such bugs is a fundamental and critical problem. However, no existing bug dataset is purposed to collect this kind of bug, posing significant obstacles to relevant research. To address this gap, we introduce TrickyBugs, a bug dataset with 3,043 buggy plausible programs sourced from human-written submissions of 324 real-world competition coding tasks. We identified the buggy plausible programs from approximately 400,000 submissions, and all the bugs in TrickyBugs were not previously detected. We hope that TrickyBugs can effectively facilitate research in the fields of automated program repair, fault localization, test generation, and test adequacy.CCS CONCEPTS•Software and its engineering → Software testing and debugging.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644870,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555821,software testing;test generation;test adequacy;program repair;benchmark,Location awareness;Software testing;Computer bugs;Production;Maintenance engineering;Software;Encoding,,,,20.0,,18 Jun 2024,,,IEEE,IEEE Conferences
GitBug-Java: A Reproducible Benchmark of Recent Java Bugs,A. Silva; N. Saavedra; M. Monperrus,"KTH Royal Institute of Technology, Stockholm, Sweden; INESC-ID/IST, University of Lisbon, Lisbon, Portugal; KTH Royal Institute of Technology, Stockholm, Sweden",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,118,122,"Bug-fix benchmarks are essential for evaluating methodologies in automatic program repair (APR) and fault localization (FL). However, existing benchmarks, exemplified by Defects4J, need to evolve to incorporate recent bug-fixes aligned with contemporary development practices. Moreover, reproducibility, a key scientific principle, has been lacking in bug-fix benchmarks. To address these gaps, we present GitBug-Java, a reproducible benchmark of recent Java bugs. GitBug-Java features 199 bugs extracted from the 2023 commit history of 55 notable open-source repositories. The methodology for building GitBug-Java ensures the preservation of bug-fixes in fully-reproducible environments. We publish GitBug-Java at https://github.com/gitbugactions/gitbug-java.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644884,Fundação para a Ciência e a Tecnologia; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555595,Software Bugs;Bug Benchmark;Reproducibility;Bug Database;Java Benchmark;Software Testing;Program Analysis,Location awareness;Java;Computer bugs;Benchmark testing;Maintenance engineering;Feature extraction;Software,,4.0,,24.0,,18 Jun 2024,,,IEEE,IEEE Conferences
P3: A Dataset of Partial Program Patches,D. Beyer; L. Grunske; M. Kettl; M. Lingsch-Rosenfeld; M. Raselimo,"LMU Munich, Munich, Germany; Humboldt-Universität zu Berlin, Berlin, Germany; LMU Munich, Munich, Germany; LMU Munich, Munich, Germany; Humboldt-Universität zu Berlin, Berlin, Germany",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,123,127,"Identifying and fixing bugs in programs remains a challenge and is one of the most time-consuming tasks in software development. But even after a bug is identified, and a fix has been proposed by a developer or tool, it is not uncommon that the fix is incomplete and does not cover all possible inputs that trigger the bug. This can happen quite often and leads to re-opened issues and inefficiencies. In this paper, we introduce P3, a curated dataset composed of incomplete fixes. Each entry in the set contains a series of commits fixing the same underlying issue, where multiple of the intermediate commits are incomplete fixes. These are sourced from real-world open-source C projects. The selection process involves both automated and manual stages. Initially, we employ heuristics to identify potential partial fixes from repositories, subsequently we validate them through meticulous manual inspection. This process ensures the accuracy and reliability of our curated dataset. We envision that the dataset will support researchers while investigating partial fixes in more detail, allowing them to develop new techniques to detect and fix them. We make our dataset publicly available at https://gitlab.com/sosy-lab/research/data/partial-fix-dataset.CCS CONCEPTS•Software and its engineering → Software testing and debugging.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644889,Deutsche Forschungsgemeinschaft; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555721,Partial Program Fixes;Supplementary Bug Fixes;Recurring Bugs,Software testing;Computer bugs;Manuals;Debugging;Inspection;Software;Software reliability,,1.0,,25.0,,18 Jun 2024,,,IEEE,IEEE Conferences
BugsPHP: A dataset for Automated Program Repair in PHP,K. D. Pramod; W. T. N. De Silva; W. U. K. Thabrew; R. Shariffdeen; S. Wickramanayake,"University of Moratuwa, Moratuwa, Sri Lanka; University of Moratuwa, Moratuwa, Sri Lanka; University of Moratuwa, Moratuwa, Sri Lanka; National University of Singapore, Singapore, Singapore; University of Moratuwa, Moratuwa, Sri Lanka",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,128,132,"Automated Program Repair (APR) improves developer productivity by saving debugging and bug-fixing time. While APR has been extensively explored for C/C++ and Java programs, there is little research on bugs in PHP programs due to the lack of a benchmark PHP bug dataset. This is surprising given that PHP has been one of the most widely used server-side languages for over two decades, being used in a variety of contexts such as e-commerce, social networking, and content management. This paper presents a benchmark dataset of PHP bugs on real-world applications called BugsPHP, which can enable research on analysis, testing, and repair for PHP programs. The dataset consists of training and test datasets, separately curated from GitHub and processed locally. The training dataset includes more than 600,000 bug-fixing commits. The test dataset contains 513 manually validated bug-fixing commits equipped with developer-provided test cases to assess patch correctness.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644878,Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555698,Automated Program Repair;PHP Application Errors,Training;Productivity;Java;Social networking (online);Computer bugs;Maintenance engineering;Benchmark testing,,1.0,,17.0,,18 Jun 2024,,,IEEE,IEEE Conferences
AW4C: A Commit-Aware C Dataset for Actionable Warning Identification,Z. Liu; M. Yan; Z. Gao; D. Li; X. Zhang; D. Yang,"School of Big Data and Software Engineering, Chongqing University, China; School of Big Data and Software Engineering, Chongqing University, China; Shanghai Institute for Advanced Study, Zhejiang University, China; School of Big Data and Software Engineering, Chongqing University, China; School of Big Data and Software Engineering, Chongqing University, China; School of Big Data and Software Engineering, Chongqing University, China",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,133,137,"Excessive non-actionable warnings generated by static program analysis tools can hinder developers from utilizing these tools effectively. Leveraging learning-based approaches for actionable warning identification has demonstrated promise in boosting developer productivity, minimizing the risk of bugs, and reducing code smells. However, the small sizes of existing datasets have limited the model choices for machine learning researchers, and the lack of aligned fix commits limits the scope of the dataset for research. In this paper, we present AW4C, an actionable warning C dataset that contains 38,134 actionable warnings mined from more than 500 repositories on GitHub. These warnings are generated via Cppcheck, and most importantly, each warning is precisely mapped to the commit where the corrective action occurred. To the best of our knowledge, this is the largest publicly available actionable warning dataset for C programming language to date. The dataset is suited for use in machine/deep learning models and can support a wide range of tasks, such as actionable warning identification and vulnerability detection. Furthermore, we have released our dataset1 and a general framework for collecting actionable warnings on GitHub2 to facilitate other researchers to replicate our work and validate their innovative ideas.CCS Concepts• Software and its engineering → Software maintenance tools; Software creation and management; Software testing and debugging;• Mathematics of computing → Data mining.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644885,National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Chongqing; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555810,Static program analysis;Actionable warning identification,Software testing;Productivity;Software maintenance;Computer languages;Codes;Computer bugs;Mathematics,,,,18.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Predicting the Impact of Crashes Across Release Channels,S. Mujahid; D. E. Costa; M. Castelluccio,"Mozilla Corporation, Montreal, Canada; Mozilla Corporation, Montreal, Canada; Mozilla Corporation, Montreal, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,138,139,"Software maintenance faces a persistent challenge with crash bugs, especially across diverse release channels catering to distinct user bases. Nightly builds, favoured by enthusiasts, often reveal crashes that are cheaper to fix but may differ significantly from those in stable releases. In this paper, we emphasize the need for a data-driven solution to predict the impact of crashes happening on nightly channels once they are released to stable channels. We also list the challenges that need to be considered when approaching this problem.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645067,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555585,,Software maintenance;Computer bugs;Data mining;Faces,,,,12.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Zero-shot Learning based Alternatives for Class Imbalanced Learning Problem in Enterprise Software Defect Analysis,S. Patil; B. Ravindran,"Tata Consultancy Services and Dept. of CSE, IIT Madras; Dept. of CSE and RBC DSAI, IIT Madras",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,140,141,Software defect reports are an important type of text data for enterprises as they provide actionable information for improving software quality. Identifying the software defect type automatically can greatly enhance and expedite defect management. Class imbalance is a real-life problem in enterprise software defect classification task and adversely affects the automation effort. We show that zero shot learning based technique can be a good alternative to the well-known supervised learning and SMOTE techniques.CCS CONCEPTS•Software and its engineering → Software defect analysis.,2574-3864,979-8-4007-0587-8,10.1145/3643991.3645065,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555746,Class Imbalance;Software Defect Analysis;Zero Shot Learning,Automation;Zero-shot learning;Supervised learning;Software quality;Software;Data mining;Task analysis,,,,8.0,,18 Jun 2024,,,IEEE,IEEE Conferences
ChatGPT Chats Decoded: Uncovering Prompt Patterns for Superior Solutions in Software Development Lifecycle,L. Wu; Y. Zhao; X. Hou; T. Liu; H. Wang,"Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China; Monash University, Melbourne, Australia; Huazhong University of Science and Technology, China",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,142,146,"The advent of Large Language Models (LLMs) like ChatGPT has markedly transformed software development, aiding tasks from code generation to issue resolution with their human-like text generation. Nevertheless, the effectiveness of these models greatly depends on the nature of the prompts given by developers. Therefore, this study delves into the DevGPT dataset, a rich collection of developer-ChatGPT dialogues, to unearth the patterns in prompts that lead to effective problem resolutions. The underlying motivation for this research is to enhance the collaboration between human developers and AI tools, thereby improving productivity and problem-solving efficacy in software development. Utilizing a combination of textual analysis and data-driven approaches, this paper seeks to identify the attributes of prompts that are associated with successful interactions, providing crucial insights for the strategic employment of ChatGPT in software engineering environments.CCS CONCEPTS•Information systems → Data mining.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645069,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555800,Data mining;Large language model;LLM;ChatGPT,Productivity;Oral communication;Chatbots;Frequency conversion;Software;Data mining;Problem-solving,,,,14.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Write me this Code: An Analysis of ChatGPT Quality for Producing Source Code,K. Moratis; T. Diamantopoulos; D. -N. Nastos; A. Symeonidis,"Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece; Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece; Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece; Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki, Thessaloniki, Greece",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,147,151,"Developers nowadays are increasingly turning to large language models (LLMs) like ChatGPT to assist them with coding tasks, inspired by the promise of efficiency and the advanced capabilities they offer. However, this raises important questions about the ease of integration and the safety of incorporating these tools into the development process. To investigate these questions, this paper examines a set of ChatGPT conversations. Upon annotating the conversations according to the intent of the developer, we focus on two critical aspects: firstly, the ease with which developers can produce suitable source code using ChatGPT, and, secondly, the quality aspects of the generated source code, determined by the compliance to standards and best practices. We research both the quality of the generated code itself and its impact on the project of the developer. Our results indicate that ChatGPT can be a useful tool for software development when used with discretion.CCS CONCEPTS• Software and its engineering → Reusability; Open source model; Software defect analysis; Software libraries and repositories.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645070,Horizon Europe; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555604,Code Generation;Code Quality;ChatGPT;Large Language Models,Codes;Source coding;Oral communication;Writing;Chatbots;Turning;Software,,2.0,,9.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Quality Assessment of ChatGPT Generated Code and their Use by Developers,M. L. Siddiq; L. Roney; J. Zhang; J. C. S. Santos,"University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA; University of Notre Dame, Notre Dame, IN, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,152,156,"The release of large language models (LLMs) like ChatGPT has revolutionized software development. Prior works explored ChatGPT’s generated response quality, the effectiveness of different prompting techniques, its performance in programming contests, etc. However, there is limited information regarding the practical usage of ChatGPT by software developers. This data mining challenge focuses on DevGPT, a curated dataset of developer-ChatGPT conversations encompassing prompts with ChatGPT’s responses, including code snippets. Our paper leverages this dataset to investigate (RQ1) whether ChatGPT generates Python & Java code with quality issues; (RQ2) whether ChatGPT-generated code is merged into a repository, and, if it does, to what extent developers change them; and (RQ3) what are the main use cases for ChatGPT besides code generation. We found that ChatGPT-generated code suffers from using undefined/unused variables and improper documentation. They also have security issues related to improper resources and exception management. Our results show that ChatGPT-generated codes are hardly merged, and they are significantly modified before merging. Based on an analysis of developers’ discussions and the developer-ChatGPT chats, we found that developers use ChatGPT for every stage of software development and leverage it to learn about new frameworks and development kits.CCS CONCEPTS• Software and its engineering → Software performance; Software usability; Empirical software validation.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645071,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555682,datasets;ChatGPT;security;quality;pull-request;open-coding,Codes;Software performance;Programming;Chatbots;Quality assessment;Data mining;Security,,11.0,,44.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Analyzing Developer Use of ChatGPT Generated Code in Open Source GitHub Projects,B. Grewal; W. Lu; S. Nadi; C. -P. Bezemer,"University of Alberta, Edmonton, Canada; University of Alberta, Edmonton, Canada; University of Alberta, Edmonton, Canada; University of Alberta, Edmonton, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,157,161,"The rapid development of large language models such as ChatGPT have made them particularly useful to developers in generating code snippets for their projects. To understand how ChatGPT’s generated code is leveraged by developers, we conducted an empirical study of 3,044 ChatGPT-generated code snippets integrated within GitHub projects. A median of 54% of the generated lines of code is found in the project’s code and this code typically remains unchanged once added. The modifications of the 76 code snippets that changed in a subsequent commit, consisted of minor functionality changes and code reorganizations that were made within a day. Our findings offer insights that help drive the development of AI-assisted programming tools. We highlight the importance of making changes in ChatGPT code before integrating it into a project.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645072,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555686,Code Reuse;LLM;SE4AI,Codes;Oral communication;Programming;Chatbots;Software;History;Data mining,,2.0,,29.0,,18 Jun 2024,,,IEEE,IEEE Conferences
How I Learned to Stop Worrying and Love ChatGPT,P. Przymus; M. Fejzer; J. Narębski; K. Stencel,"Nicolaus Copernicus University in Toruń, Poland; Nicolaus Copernicus University in Toruń, Poland; Nicolaus Copernicus University in Toruń, Poland; University of Warsaw, Poland",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,162,166,"In the dynamic landscape of software engineering, the emergence of ChatGPT-generated code signifies a distinctive and evolving paradigm in development practices. We delve into the impact of interactions with ChatGPT on the software development process, specifically analysing its influence on source code changes. Our emphasis lies in aligning code with ChatGPT conversations, separately analysing the user-provided context of the code and the extent to which the resulting code has been influenced by ChatGPT. Additionally, employing survival analysis techniques, we examine the longevity of ChatGPT-generated code segments in comparison to lines written traditionally. The goal is to provide valuable insights into the transformative role of ChatGPT in software development, illuminating its implications for code evolution and sustainability within the ecosystem.CCS CONCEPTS• Software and its engineering → Software prototyping; Software evolution; Automatic programming.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645073,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555717,ChatGPT;DevGPT;MSR;Code Survival Analysis,Software prototyping;Codes;Source coding;Oral communication;Chatbots;Software;Maintenance,,,,17.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation,K. Jin; C. -Y. Wang; H. V. Pham; H. Hemmati,"York University, Toronto, Canada; York University, Toronto, Canada; York University, Toronto, Canada; York University, Toronto, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,167,171,"Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers’ conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645074,Alberta Innovates; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555687,,Codes;Refining;Collaboration;Oral communication;Documentation;Chatbots;Software,,1.0,,26.0,,18 Jun 2024,,,IEEE,IEEE Conferences
The role of library versions in Developer-ChatGPT conversations,R. Raj; D. E. Costa,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,172,176,"The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks. Developers use ChatGPT to write code, review code changes, and even debug their programs. In these interactions, ChatGPT often recommends code snippets that depend on external libraries. However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries. Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses. Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645075,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555730,,Vocabulary;Codes;Reviews;Oral communication;Chatbots;Libraries;Software,,,,23.0,,18 Jun 2024,,,IEEE,IEEE Conferences
"AI Writes, We Analyze: The ChatGPT Python Code Saga",M. F. Rabbi; A. Champa; M. Zibran; M. R. Islam,"Idaho State University, USA; Idaho State University, USA; Idaho State University, USA; Lamar University, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,177,181,"In this study, we quantitatively analyze 1,756 AI-written Python code snippets in the DevGPT dataset and evaluate them for quality and security issues. We systematically distinguish the code snippets as either generated by ChatGPT from scratch (ChatGPT-generated) or modified user-provided code (ChatGPT-modified). The results reveal that ChatGPT-modified code more frequently displays quality issues compared to ChatGPT-generated code. The findings provide insights into the inherent limitations of AI-written code and emphasize the need for scrutiny before integrating such pieces of code into software systems.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645076,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555820,Code quality;Code security;ChatGPT;Python;CWE;Analysis,Measurement;Codes;Source coding;Oral communication;Chatbots;Software systems;Security,,9.0,,44.0,,18 Jun 2024,,,IEEE,IEEE Conferences
ChatGPT in Action: Analyzing Its Use in Software Development,A. I. Champa; M. F. Rabbi; C. Nachuma; M. F. Zibran,"Department of Computer Science, Idaho State University, Pocatello, ID, USA; Department of Computer Science, Idaho State University, Pocatello, ID, USA; Department of Computer Science, Idaho State University, Pocatello, ID, USA; Department of Computer Science, Idaho State University, Pocatello, ID, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,182,186,"The emergence of AI tools such as ChatGPT is being used to assist with software development, but little is known of how developers utilize these tools as well as the capabilities of these tools in software engineering tasks. Using the DevGPT dataset, we conduct quantitative analyses of the tasks developers seek assistance from ChatGPT and how effectively ChatGPT addresses them. We also examine the impact of initial prompt quality on conversation length. The findings reveal where ChatGPT is most and least suited to assist in the identified 12 software development tasks. The insights from this research would guide the software developers, researchers, and AI tool providers in optimizing these tools for more effective programming aid.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645077,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555598,ChatGPT conversation;software development tasks;task efficiency;prompt quality,Codes;Statistical analysis;Oral communication;Chatbots;Software;Task analysis;Artificial intelligence,,6.0,,49.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Chatting with AI: Deciphering Developer Conversations with ChatGPT,S. Mohamed; A. Parvin; E. Parra,"Belmont University, Nashville, TN, USA; Belmont University, Nashville, TN, USA; Belmont University, Nashville, TN, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,187,191,"Large Language Models (LLMs) have been widely adopted and are becoming ubiquitous and integral to software development. However, we have little knowledge as to how these tools are being used by software developers beyond anecdotal evidence and word-of-mouth reports. In this work, we present a study toward understanding how developers engage with and utilize LLMs by reporting the results of an empirical study identifying patterns in the conversation that developers have with LLMs. We identified a total of 19 topics describing the purpose of the developers in their conversations with LLMs. Our findings reveal that developers use LLMs to facilitate various aspects of their software development processes (e.g., information-seeking about programming languages and frameworks and soliciting high-level design recommendations) to a similar extent to which they use them for non-development purposes such as writing assistance, general purpose queries, and conducting Turing tests to assess the intrinsic capabilities of the models. This work not only sheds light on the diverse applications of LLMs in software development but also underscores their emerging role as critical tools in enhancing developer productivity and creativity as we move closer to widespread AI-assisted software development.CCS CONCEPTS• General and reference → Empirical studies; • Information systems → Data mining; • Computing methodologies → Artificial intelligence; Distributed artificial intelligence; • Software and its engineering;",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645078,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555805,Large Language Models;LLM;chatGPT;Software Development;Empirical Study;Developer conversations,Codes;Oral communication;Writing;Chatbots;Software;Encoding;Data mining,,2.0,,23.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Does Generative AI Generate Smells Related to Container Orchestration?: An Exploratory Study with Kubernetes Manifests,Y. Zhang; R. Meredith; W. Reeves; J. Coriolano; M. A. Babar; A. Rahman,"Auburn University, Auburn, Alabama, USA; Auburn University, Auburn, Alabama, USA; Auburn University, Auburn, Alabama, USA; Federal University of Pernambuco, Recife, Brazil; University of Adelaide, Adelaide, Australia; Auburn University, Auburn, Alabama, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,192,196,"Generative artificial intelligence (AI) technologies, such as Chat-GPT have shown promise in solving software engineering problems. However, these technologies have also shown to be susceptible to generating software artifacts that contain quality issues. A systematic characterization of quality issues, such as smells in ChatGPT-generated artifacts can help in providing recommendations for practitioners who use generative AI for container orchestration.We conduct an empirical study with 98 Kubernetes manifests to quantify smells in manifests generated by ChatGPT. Our empirical study shows: (i) 35.8% of the 98 Kubernetes manifests generated include at least one instance of smell; (ii) two types of objects Kubernetes namely, Deployment and Service are impacted by identified smells; and (iii) the most frequently occurring smell is unset CPU and memory requirements. Based on our findings, we recommend practitioners to apply quality assurance activities for ChatGPT-generated Kubernetes manifests prior to using these manifests for container orchestration.CCS CONCEPTS• Software and its engineering → Software verification and validation; Software defect analysis.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645079,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555616,container orchestration;empirical study;kubernetes;quality;smell,Systematics;Generative AI;Memory management;Static analysis;Containers;Chatbots;Software,,2.0,,24.0,,18 Jun 2024,,,IEEE,IEEE Conferences
On the Taxonomy of Developers’ Discussion Topics with ChatGPT,E. Sagdic; A. Bayram; M. R. Islam,"Lamar University, Beaumont, Texas, USA; Lamar University, Beaumont, Texas, USA; Lamar University, Beaumont, Texas, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,197,201,"Large language models (LLMs) like ChatGPT can generate text for various prompts. With exceptional reasoning capabilities, ChatGPT (particularly the GPT-4 model) has achieved widespread adoption across many tasks - from creative writing to domain-specific inquiries, code generation, and more. This research analyzed the DevGPT dataset to determine common topics posed by developers interacting with ChatGPT. The DevGPT dataset comprises ChatGPT interactions from GitHub issues, pull requests and discussions. By employing a mixed-methods approach combining unsupervised semantic modeling and expert qualitative analysis we categorize the topics developers discuss when interacting with ChatGPT.Our approach reveals 17 topics within seven categories, with over 25% of prompts focused on advanced programming guidance. Additional areas of significant query volume include DevOps workflows, SQL, databases, and specialized domains, such as localization, streaming media, and image processing. This research effectively illuminates core topics and dependencies that motivate developers to leverage ChatGPT. The taxonomy classification further clarifies critical areas to better customize AI tools for aligning with workflows and needs within software engineering contexts.CCS CONCEPTS• Software and its engineering → Programming teams.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645080,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555669,DevGPT;ChatGPT;Software Engineering;Topic Taxonomy,Analytical models;DevOps;Taxonomy;Semantics;Programming;Chatbots;Software,,,,32.0,,18 Jun 2024,,,IEEE,IEEE Conferences
How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations,E. A. AlOmar; A. Venkatakrishnan; M. W. Mkaouer; C. D. Newman; A. Ouni,"Stevens Institute of Technology, Hoboken, New Jersey, USA; Rochester Institute of Technology, Rochester, New York, USA; University of Michigan-Flint, Flint, Michigan, USA; Rochester Institute of Technology, Rochester, New York, USA; ETS Montreal, University of Quebec, Montreal, Quebec, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,202,206,"Large Language Models (LLMs), like ChatGPT, have gained widespread popularity and usage in various software engineering tasks, including refactoring, testing, code review, and program comprehension. Despite recent studies delving into refactoring documentation in commit messages, issues, and code review, little is known about how developers articulate their refactoring needs when interacting with ChatGPT. In this paper, our goal is to explore conversations between developers and ChatGPT related to refactoring to better understand how developers identify areas for improvement in code and how ChatGPT addresses developers’ needs. Our approach relies on text mining refactoring-related conversations from 17,913 ChatGPT prompts and responses, and investigating developers’ explicit refactoring intention. Our results reveal that (1) developer-ChatGPT conversations commonly involve generic and specific terms/phrases; (2) developers often make generic refactoring requests, while ChatGPT typically includes the refactoring intention; and (3) various learning settings when prompting ChatGPT in the context of refactoring. We envision that our findings contribute to a broader understanding of the collaboration between developers and AI models.CCS CONCEPTS• Software Engineering → Software Quality; Refactoring.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645081,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555806,Refactoring documentation;ChatGPT;mining software repositories,Text mining;Codes;Reviews;Oral communication;Software quality;Documentation;Chatbots,,4.0,,45.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Analyzing Developer-ChatGPT Conversations for Software Refactoring: An Exploratory Study,S. Deo; D. Hinge; O. S. Chavan; Y. Olivia Wang; M. W. Mkaouer,"Rochester Institute of Technology, Rochester, New York, USA; Rochester Institute of Technology, Rochester, New York, USA; Rochester Institute of Technology, Rochester, New York, USA; Rochester Institute of Technology, Rochester, New York, USA; University of Michigan-Flint, Flint, Michigan, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,207,211,"In recent years, Large Language Models (LLMs) have witnessed a remarkable ascent, with OpenAI’s ChatGPT, introduced in 2022, garnering substantial attention. ChatGPT’s rapid adoption in the software development community has opened up new avenues for exploring its qualitative and quantitative impact on Developer-ChatGPT conversations. In this paper, we delve into a rich dataset from GitHub and Hacker News to perform a thorough analysis. Our objectives include characterizing the nature of these interactions and evaluating the use of ChatGPT in refactoring. To achieve these goals, we employ a combination of exploratory data analysis and data annotation, utilizing relevant keyword filters to extract pertinent information. Our examination encompasses the identification and analysis of code refactorings facilitated by ChatGPT. Through a meticulous exploration of these conversations, our goal is to illuminate the potential of ChatGPT to enhance software development practices. This research promises to provide valuable insights into the evolving role of ChatGPT in the world of software development.CCS CONCEPTS• Software Engineering → Software Quality; Refactoring.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645082,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555866,Refactoring documentation;ChatGPT;mining software repositories,Codes;Filters;Focusing;Oral communication;Software quality;Chatbots;Data mining,,,,15.0,,18 Jun 2024,,,IEEE,IEEE Conferences
How Do So ware Developers Use ChatGPT? An Exploratory Study on GitHub Pull Requests,M. Chouchen; N. Bessghaier; M. Begoug; A. Ouni; E. A. AlOmar; M. Wiem Mkaouer,"ETS Montreal, University of Quebec; ETS Montreal, University of Quebec; ETS Montreal, University of Quebec; ETS Montreal, University of Quebec; Stevens Institute of Technology; University of Michigan-Flint",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,212,216,"Nowadays, Large Language Models (LLMs) play a pivotal role in software engineering. Developers can use LLMs to address software development-related tasks such as documentation, code refactoring, debugging, and testing. ChatGPT, released by OpenAI, has become the most prominent LLM. In particular, ChatGPT is a cutting-edge tool for providing recommendations and solutions for developers in their pull requests (PRs). However, little is known about the characteristics of PRs that incorporate ChatGPT compared to those without it and what developers usually use it for. To this end, we quantitatively analyzed 243 PRs that listed at least one ChatGPT prompt against a representative sample of 384 PRs without any ChatGPT prompts. Our findings show that developers use ChatGPT in larger, time-consuming pull requests that are five times slower to be closed than PRs that do not use ChatGPT. Furthermore, we perform a qualitative analysis to build a taxonomy of the topics developers primarily address in their prompts. Our analysis results in a taxonomy comprising 8 topics and 32 sub-topics. Our findings highlight that ChatGPT is often used in review-intensive pull requests. Moreover, our taxonomy enriches our understanding of the developer’s current applications of ChatGPT.CCS CONCEPTS• Software and its engineering → Collaboration in software development.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645084,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555786,Large Language Models;ChatGPT;Manual analysis;Mining Software Repositories;Pull Requests,Taxonomy;Documentation;Debugging;Chatbots;Software;Data mining;Task analysis,,1.0,,43.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study,J. K. Das; S. Mondal; C. K. Roy,"University of Saskatchewan, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,217,221,"Issue tracking systems serve as the primary tool for incorporating external users and customizing a software project to meet the users’ requirements. However, the limited number of contributors and the challenge of identifying the best approach for each issue often impede effective resolution. Recently, an increasing number of developers are turning to AI tools like ChatGPT to enhance problem-solving efficiency. While previous studies have demonstrated the potential of ChatGPT in areas such as automatic program repair, debugging, and code generation, there is a lack of study on how developers explicitly utilize ChatGPT to resolve issues in their tracking system. Hence, this study aims to examine the interaction between ChatGPT and developers to analyze their prevalent activities and provide a resolution. In addition, we assess the code reliability by confirming if the code produced by ChatGPT was integrated into the project’s codebase using the clone detection tool NiCad. Our investigation reveals that developers mainly use ChatGPT for brainstorming solutions but often opt to write their code instead of using ChatGPT-generated code, possibly due to concerns over the generation of ""hallucinated"" code, as highlighted in the literature.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645083,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555858,ChatGPT;Issue Tracking;NiCad;Code Clone,Codes;Cloning;Debugging;Maintenance engineering;Chatbots;Turning;Software,,1.0,,25.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution,S. Mondal; S. D. Bappon; C. K. Roy,"University of Saskatchewan, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,222,226,"Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model’s ability to extract contextually accurate responses. Thus, optimal prompt construction is essential for maximizing the utility and performance of ChatGPT. However, suboptimal prompt design may necessitate iterative refinement, as imprecise or ambiguous instructions can lead to undesired responses from ChatGPT. Existing studies explore several prompt patterns and strategies to improve the relevance of responses generated by ChatGPT. However, the exploration of constraints that necessitate the submission of multiple prompts is still an unmet attempt. In this study, our contributions are twofold. First, we attempt to uncover gaps in prompt design that demand multiple iterations. In particular, we manually analyze 686 prompts that were submitted to resolve issues related to Java and Python programming languages and identify eleven prompt design gaps (e.g., missing specifications). Such gap exploration can enhance the efficacy of single prompts in ChatGPT. Second, we attempt to reproduce the ChatGPT response by consolidating multiple prompts into a single one. We can completely consolidate prompts with four gaps (e.g., missing context) and partially consolidate prompts with three gaps (e.g., additional functionality). Such an effort provides concrete evidence to users to design more optimal prompts mitigating these gaps. Our study findings and evidence can – (a) save users time, (b) reduce costs, and (c) increase user satisfaction.CCS CONCEPTS• Large Language Model → Prompt design; Prompt characterization.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645085,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555707,Prompt design;ChatGPT;prompt consolidation;qualitative analysis,Java;Costs;Accuracy;Chatbots;Software;Iterative methods;Data mining,,,,18.0,,18 Jun 2024,,,IEEE,IEEE Conferences
DevGPT: Studying Developer-ChatGPT Conversations,T. Xiao; C. Treude; H. Hata; K. Matsumoto,"Nara Institute of Science and Technology, Japan; University of Melbourne, Australia; Shinshu University, Japan; Nara Institute of Science and Technology, Japan",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,227,230,"This paper introduces DevGPT, a dataset curated to explore how software developers interact with ChatGPT, a prominent large language model (LLM). The dataset encompasses 29,778 prompts and responses from ChatGPT, including 19,106 code snippets, and is linked to corresponding software development artifacts such as source code, commits, issues, pull requests, discussions, and Hacker News threads. This comprehensive dataset is derived from shared ChatGPT conversations collected from GitHub and Hacker News, providing a rich resource for understanding the dynamics of developer interactions with ChatGPT, the nature of their inquiries, and the impact of these interactions on their work. DevGPT enables the study of developer queries, the effectiveness of ChatGPT in code generation and problem solving, and the broader implications of AI-assisted programming. By providing this dataset, the paper paves the way for novel research avenues in software engineering, particularly in understanding and improving the use of LLMs like ChatGPT by developers.CCS CONCEPTS• Information systems → Data mining.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3648400,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555646,ChatGPT;LLM;Generative AI;dataset,Codes;Computer hacking;Source coding;Oral communication;Programming;Chatbots;Software,,24.0,,41.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Not all Dockerfile Smells are the Same: An Empirical Evaluation of Hadolint Writing Practices by Experts,G. Rosa; S. Scalabrino; G. Robles; R. Oliveto,"STAKE Lab, University of Molise, Pesche, Italy; STAKE Lab, University of Molise, Pesche, Italy; Universidad Rey Juan Carlos, Madrid, Spain; STAKE Lab, University of Molise, Pesche, Italy",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,231,241,"Dockerfiles can be affected by bad design choices, known as Dock-erfile smells. Hadolint is currently the reference tool able to detect them, and it is widely used both by researchers and practitioners. The literature shows that these smells are commonly diffused in Dockerfiles, but it is still not clear how developers perceive them as bad practices. This paper aims to investigate the relevance of the Dockerfile smells captured by hadolint from the perspective of expert Dockerfile developers. We first perform a mining study in which we extract the change history of Dockerfiles maintained by experts to understand what smells have been more frequently introduced in their history. Next, we ran a survey in which we asked expert Dockerfile developers to evaluate Dockerfiles affected by different smells. We obtained 94 responses for 17 smells, representative of 24 Dockerfile smells. We found that experts prioritize a small part of the evaluated smells over others. Besides, they report additional bad practices not mapped as smells in any existing catalog. Thus, we propose a ranked catalog containing 26 additional Docker-file smells, which can be used as a guide for novices to understand which aspects to focus on to write good-quality Dockerfiles.CCS CONCEPTS• Software and its engineering → Software notations and tools.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644905,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555789,Empirical Software Engineering;Docker;Code Smells,Surveys;Writing;Software;History;Data mining,,2.0,,39.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Supporting High-Level to Low-Level Requirements Coverage Reviewing with Large Language Models,A. -R. Preda; C. Mayr-Dorn; A. Mashkoor; A. Egyed,"Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,242,253,"Refining high-level requirements into low-level ones is a common task, especially in safety-critical systems engineering. The objective is to describe every important aspect of the high-level requirement in a low-level requirement, ensuring a complete and correct implementation of the system’s features. To this end, standards and regulations for safety-critical systems require reviewing the coverage of high-level requirements by all its low-level requirements to ensure no missing aspects.The challenge of supporting automatic reviews for requirements coverage originates from the distinct levels of abstraction between high-level and low-level requirements, their reliance on natural language, and the often different vocabulary used. The rise of Large Language Models (LLMs), trained on extensive text corpora and capable of contextualizing both high-level and low-level requirements, opens new avenues for addressing this challenge.This paper presents an initial study to explore the performance of LLMs in assessing requirements coverage. We employed GPT-3.5 and GPT-4 to analyze requirements from five publicly accessible data sets, determining their ability to detect if low-level requirements sufficiently address the corresponding high-level requirement. Our findings reveal that GPT-3.5, utilizing a zero-shot prompting strategy augmented with the prompt of explaining, correctly identifies complete coverage in four out of five evaluation data sets. Additionally, it exhibits an impressive 99.7% recall rate in accurately identifying instances where coverage is incomplete due to removing a single low-level requirement across our entire set of evaluation data.CCS CONCEPTS• Software and its engineering → Software creation and management; Designing software; Requirements analysis.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644922,Austrian Science Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555592,Coverage;Traceability;Requirements;Design Definitions;High-level Requirements;Low-level Requirements;Requirements Satisfaction Assessment;Large Language Models;GPT,Vocabulary;Reviews;Refining;Natural languages;Software;Regulation;Data mining,,1.0,,43.0,,18 Jun 2024,,,IEEE,IEEE Conferences
On the Executability of R Markdown Files,M. A. Islam; M. Asaduzzman; S. Wang,"Lakehead University, Thunder Bay, Canada; University of Windsor, Windsor, Canada; University of Manitoba, Winnipeg, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,254,264,"R Markdown files are examples of literate programming documents that combine R code with results and explanations. Such dynamic documents are designed to execute easily and reproduce study results. However, little is known about the executability of R Markdown files which can cause frustration among its users who intend to reuse the document. This paper presents a large-scale study on the executability of R Markdown files collected from GitHub. Results from our study show that a significant number of R Markdown files (64.95%) are not executable, even after our best efforts. To better understand the challenges, we categorize the exceptions encountered while executing the documents into different categories. Finally, we develop a classifier to determine which Markdown files are likely to be executable. Such a classifier can be utilized by search engines in their ranking which helps developers to find literate programming documents as learning resources.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644931,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555677,R Markdown;GitHub;Executability;Literate Programming,Codes;Accuracy;Ecosystems;Manuals;Programming;Search engines;Software,,1.0,,42.0,,18 Jun 2024,,,IEEE,IEEE Conferences
APIstic: A Large Collection of OpenAPI Metrics,S. Serbout; C. Pautasso,"Software Institute, USI, Lugano, Switzerland; Software Institute, USI, Lugano, Switzerland",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,265,277,"In the rapidly evolving landscape of web services, the significance of eﬃciently designed and well-documented APIs is paramount. In this paper, we present APIstic an API analytics dataset and exploration tool to navigate and segment APIs based on an extensive set of pre-computed metrics extracted from OpenAPI specifications, sourced from GitHub, SwaggerHub, BigQuery and APIs.guru. These pre-computed metrics are categorized into structure, data model, natural language description, and security metrics. The extensive dataset of varied API metrics provides crucial insights into API design and documentation for both researchers and practitioners. Researchers can use APIstic as an empirical resource to extract refined samples, analyze API design trends, best practices, smells, and patterns. For API designers, it serves as a benchmarking tool to assess, compare, and improve API structures, data models, and documentation using metrics to select points of references among 1,275,568 valid OpenAPI specifications. The paper discusses potential use cases of the collected data and presents a descriptive analysis of selected API analytics metrics.The dataset available at: http://openapi.inf.usi.ch/",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644932,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555845,Web APIs;Dataset;OpenAPI;API Metrics,Measurement;Analytical models;Web services;Documentation;Market research;Particle measurements;Software,,2.0,,74.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Improving Automated Code Reviews: Learning from Experience,H. Y. Lin; P. Thongtanunam; C. Treude; W. Charoenwet,"The University of Melbourne, Melbourne, Australia; The University of Melbourne, Melbourne, Australia; Singapore Management University, Singapore, Singapore; The University of Melbourne, Melbourne, Australia",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,278,283,"Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would. A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-eﬃcient ways to boost automated code review models.CCS CONCEPTS• Software creation and management; • Machine translation;",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644910,University of Melbourne; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555781,Code Review;Review Comments;Neural Machine Translation,Training;Industries;Codes;Quality assurance;Reviews;Training data;Data models,,3.0,,39.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Multi-faceted Code Smell Detection at Scale using DesigniteJava 2.0,T. Sharma,"Dalhousie University, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,284,288,"Code smell detection tools not only help practitioners and researchers detect maintainability issues but also enable repository mining and empirical research involving code smells. However, current tools for detecting code smells exhibit notable shortcomings, such as limited coverage for a diverse kind of smells at varying granularities, lack of maintenance, and inadequate support for large-scale mining studies. To address the limitations, the first major version of DesigniteJava supported code smells detection at architecture, design, and implementation smells along with commonly used code quality metrics. This paper presents DesigniteJava 2.0 that adds testability and test smell detection support. Also, the tool offers new analysis modes, including an optimized multi-commit analysis mode, to support large-scale multi-commit analysis. We show that the optimized multi-commit mode reduces analysis time by up to 46% without compromising the analysis efficacy. The tool is available online. Replication package including all the validation data and scripts can be found online [27]. Demonstration video can be found on YouTube.CCS CONCEPTS• Software and its engineering → Maintaining software; Software maintenance tools.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644881,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555873,Code smell detection tool;repository mining,Measurement;Software maintenance;Codes;Computer architecture;Maintenance;Data mining;Software engineering,,1.0,,40.0,,18 Jun 2024,,,IEEE,IEEE Conferences
SATDAUG - A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt,E. Sutoyo; A. Capiluppi,"Bernoulli Institute, University of Groningen, Groningen, Netherlands; Bernoulli Institute, University of Groningen, Groningen, Netherlands",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,289,293,"Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD identification (i.e., whether an instance is SATD or not) and categorization (i.e., which type of SATD is being classified) in existing datasets, we share the SATDAUG dataset, an augmented version of existing SATD datasets, including source code comments, issue tracker, pull requests, and commit messages. These augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644880,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555726,self-admitted technical debt;data augmentation;class imbalance,Training;Deep learning;Accuracy;Source coding;Training data;Data models;Software,,,,32.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Curated Email-Based Code Reviews Datasets,M. Liang; W. Charoenwet; P. Thongtanunam,"The University of Melbourne, Australia; The University of Melbourne, Australia; The University of Melbourne, Australia",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,294,298,"Code review is an important practice that improves the overall quality of a proposed patch (i.e. code changes). While much research focused on tool-based code reviews (e.g. a Gerrit code review tool, GitHub), many traditional open-source software (OSS) projects still conduct code reviews through emails. However, due to the nature of unstructured email-based data, it can be challenging to mine email-based code reviews, hindering researchers from delving into the code review practice of such long-standing OSS projects. Therefore, this paper presents large-scale datasets of email-based code reviews of 167 projects across three OSS communities (i.e. Linux Kernel, OzLabs, and FFmpeg). We mined the data from Patchwork, a web-based patch-tracking system for email-based code review, and curated the data by grouping a submitted patch and its revised versions and grouping email aliases. Our datasets include a total of 4.2M patches with 2.1M patch groups and 169K email addresses belonging to 141K individuals. Our published artefacts include the datasets as well as a tool suite to crawl, curate, and store Patch-work data. With our datasets, future work can directly delve into an email-based code review practice of large OSS projects without additional effort in data collection and curation.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644872,Australian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555631,,Codes;Reviews;Linux;Data collection;Data mining;Kernel;Open source software,,,,37.0,,18 Jun 2024,,,IEEE,IEEE Conferences
TestDossier: A Dataset of Tested Values Automatically Extracted from Test Execution,A. Hora,"Department of Computer Science, UFMG, Belo Horizonte, Brazil",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,299,303,"Real-world test suites are often complex and may have thousands of test cases. In this scenario, it is not easy to spot what values are actually covered by the tests. Having access to every tested value of a test suite would provide the basis to (1) assess the quality of tested data and (2) have actionable information to improve them. In this paper, we propose TestDossier, a dataset of tested values automatically extracted from the execution of Python tests. To collect runtime data, we run an instrumented version of the tests, monitoring the test execution, and extracting argument and variable values. We monitored the test suites of 15 Python Standard Libraries to create the dataset. TestDossier contains 1,234 distinct argument/variable names and 133,169 distinct values, leading to a total of 12,9M individual values. We envision that our dataset can help developers detect rarely tested values, untested values, and variations of tested values. We also foresee that our dataset can support novel empirical studies in the context of software testing, for example, it can expose the diversity of the tested data.CCS CONCEPTS• Software and its engineering → Software testing and debugging; Runtime environments.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644875,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555869,software testing;test quality;python;unittest;pytest,Software testing;Runtime environment;Instruments;Software;Libraries;Generators;Data mining,,1.0,,28.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Greenlight: Highlighting TensorFlow APIs Energy Footprint,S. Rajput; M. Kechagia; F. Sarro; T. Sharma,"Dalhousie University, Canada; University College, London, UK; University College, London, UK; Dalhousie University, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,304,308,"Deep learning (dl) models are being widely deployed in real-world applications, but their usage remains computationally intensive and energy-hungry. While prior work has examined model-level energy usage, the energy footprint of the dl frameworks, such as TensorFlow and PyTorch, used to train and build these models, has not been thoroughly studied. We present Greenlight, a large-scale dataset containing fine-grained energy profiling information of 1284 TensorFlow api calls. We developed a command line tool called CodeGreen to curate such a dataset. CodeGreen is based on our previously proposed framework FECoM, which employs static analysis and code instrumentation to isolate invocations of TensorFlow operations and measure their energy consumption precisely. By executing api calls on representative workloads and measuring the consumed energy, we construct detailed energy profiles for the apis. Several factors, such as input data size and the type of operation, significantly impact energy footprints. Greenlight provides a ground-truth dataset capturing energy consumption along with relevant factors such as input parameter size to take the first step towards optimization of energy-intensive TensorFlow code. The Greenlight dataset opens up new research directions such as predicting api energy consumption, automated optimization, modeling efficiency trade-offs, and empirical studies into energy-aware dl system design.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644894,European Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555878,Energy measurement;Green Artificial Intelligence;Fine-grained energy measurement,Energy consumption;Codes;Computational modeling;Instruments;Energy measurement;Static analysis;Predictive models,,,,20.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Automating GUI-based Test Oracles for Mobile Apps,K. Baral; J. Johnson; J. Mahmud; S. Salma; M. Fazzini; J. Rubin; J. Offutt; K. Moran,"CQSE America, Sunnyvale, CA, USA; University of Minnesota, Minneapolis, MN, USA; University of Central Florida, Orlando, FL, USA; George Mason University, Fairfax, VA, USA; University of Minnesota, Minneapolis, MN, USA; The University of British Columbia, Vancouver, Canada; University of Albany, Albany, NY, USA; University of Central Florida, Orlando, FL, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,309,321,"In automated testing, test oracles are used to determine whether software behaves correctly on individual tests by comparing expected behavior with actual behavior, revealing incorrect behavior. Automatically creating test oracles is a challenging task, especially in domains where software behavior is difficult to model. Mobile apps are one such domain, primarily due to their event-driven, GUI-based nature, coupled with significant ecosystem fragmentation.This paper takes a step toward automating the construction of GUI-based test oracles for mobile apps, first by characterizing common behaviors associated with failures into a behavioral taxonomy, and second by using this taxonomy to create automated oracles. Our taxonomy identifies and categorizes common GUI element behaviors, expected app responses, and failures from 124 reproducible bug reports, which allow us to better understand oracle characteristics. We use the taxonomy to create app-independent oracles and report on their generalizability by analyzing an additional dataset of 603 bug reports. We also use this taxonomy to define an app-independent process for creating automated test oracles, which leverages computer vision and natural language processing, and apply our process to automate five types of app-independent oracles. We perform a case study to assess the effectiveness of our automated oracles by exposing them to 15 real-world failures. The oracles reveal 11 of the 15 failures and report only one false positive. Additionally, we combine our oracles with a recent automated test input generation tool for Android, revealing two bugs with a low false positive rate. Our results can help developers create stronger automated tests that can reveal more problems in mobile apps and help researchers who can use the understanding from the taxonomy to make further advances in test automation.CCS CONCEPTS• Software and its engineering → Software maintenance tools; Software testing and debugging; Software usability.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644930,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555784,Mobile apps;Test Oracles;Software Testing;UI Analysis,Software testing;Software maintenance;Taxonomy;Computer bugs;Ecosystems;Natural language processing;Mobile applications,,1.0,,69.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Global Prosperity or Local Monopoly? Understanding the Geography of App Popularity,L. Wang; C. Zheng; H. Wang; X. Luo; G. Tyson; Y. Wang; S. Wang,"Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Huazhong University of Science and Technology, Wuhan, China; The Hong Kong Polytechnic University, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,322,334,"App stores allow developers to globally distribute their apps to gain more users and attention. In the highly competitive market of app stores, developers need to cater to a large number of users spanning multiple countries. We posit that the characteristics of diverse geographical, linguistic, cultural, societal, and economic environments may impact the adoption of apps. In this paper, we take the first step to characterize popular apps across over 150 countries worldwide, and explore the potential correlations to a number of underlying factors including geography, language as well as cultural, societal, and economic dimensions. Our study is based on a longitudinal (one-year) dataset of daily app popularity from the iOS app stores, covering 154 regions around the world. We reveal that app popularity shows great diversity across the world, while similarities exist among countries that share geographical proximity and linguistic convergence. The differences in app popularity across regions can be further correlated with the cultural model and socioeconomic indices we adopt. On top of the dataset and findings, we implement a prediction task that contributes to app distribution, helping developers choose the right market to distribute and promote their apps. To the best of our knowledge, we are the first to attempt to provide a global understanding of the characteristics of app popularity across the mobile app ecosystem. Our observations can benefit stakeholders in the ecosystem, striving to improve app uptake.CCS CONCEPTS • Human-centered computing → Human computer interaction(HCI).",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644935,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555791,App popularity;Cross country;iOS;App store,Geography;Ecosystems;Linguistics;Software;Monopoly;Cultural differences;Stakeholders,,,,53.0,,18 Jun 2024,,,IEEE,IEEE Conferences
GuiEvo: Automated Evolution of Mobile Application GUIs,S. Salma; S. H. Mansur; Y. Zhang; K. Moran,"George Mason University, Fairfax, VA, USA; George Mason University, Fairfax, VA, USA; George Mason University, Fairfax, VA, USA; University of Central, Florida",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,335,347,"With the increasing use of mobile applications in today’s digital world, touch-based graphical user interfaces (GUIs) have become a crucial component of modern software by which end-users carry out computing tasks. As such, the tools involved in creating these GUIs are of fundamental importance. Due to the continuous pressure for frequent releases of mobile apps to keep pace with platform and device updates, the practice of evolving app GUIs is central to mobile app maintenance. Currently, developers manually introduce GUI changes to their apps as they evolve in a time-consuming process that involves creating mock-ups of updated GUIs and then implementing the changes stipulated by the mock-up.To help ease the burden of implementing GUI changes, and to help free mobile app developers to focus on fixing bugs or adding features, this paper introduces an automated approach for GUI evolution, called GuiEvo. This approach aims to assist developers in the process of GUI evolution by detecting changes in GUIs between existing releases and proposed mock-ups using computer vision techniques, and automatically generating updated GUI metadata for the new release. We evaluate our approach’s performance based on accuracy, precision, recall, and F1-score in detecting the GUI changes, and tree edit distance to measure the correctness of generated UI hierarchies. Our evaluation demonstrates that GuiEvo can detect GUI changes with over 85% accuracy, and the generated GUI hierarchies closely match the expected structure with an average tree edit distance of 5.9. This work points toward the promise of automated tool support for assisting in the evolution of GUIs.CCS CONCEPTS• Software and its engineering → Software maintenance tools; Software usability.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644936,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555828,Mobile apps;UI Analysis;UI Generation;UI Maintenance,Software maintenance;Accuracy;Operating systems;Metadata;Mobile applications;Maintenance;Data mining,,1.0,,56.0,,18 Jun 2024,,,IEEE,IEEE Conferences
"Comparing Apples to Androids: Discovery, Retrieval, and Matching of iOS and Android Apps for Cross-Platform Analyses",M. Steinböck; J. Bleier; M. Rainer; T. Urban; C. Utz; M. Lindorfer,"TU Wien, Vienna, Austria; TU Wien, Vienna, Austria; CISPA Helmholtz Center for Information Security, Saarbrücken, Germany; Institute for Internet Security, Gelsenkirchen, Germany; CISPA Helmholtz Center for Information Security, Saarbrücken, Germany; TU Wien, Vienna, Austria",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,348,360,"For years, researchers have been analyzing mobile Android apps to investigate diverse properties such as software engineering practices, business models, security, privacy, or usability, as well as differences between marketplaces. While similar studies on iOS have been limited, recent work has started to analyze and compare Android apps with those for iOS. To obtain the most representative analysis results across platforms, the ideal approach is to compare their characteristics and behavior for the same set of apps, e. g., to study a set of apps for iOS and their respective counterparts for Android. Previous work has only attempted to identify and evaluate such cross-platform apps to a limited degree, mostly comparing sets of apps independently drawn from app stores, manually matching small sets of apps, or relying on brittle matches based on app and developer names. This results in (1) comparing apps whose behavior and properties significantly differ, (2) limited scalability, and (3) the risk of matching only a small fraction of apps.In this work, we propose a novel approach to create an extensive dataset of cross-platform apps for the iOS and Android ecosystems. We describe an analysis pipeline for discovering, retrieving, and matching apps from the Apple App Store and Google Play Store that we used to create a set of 3,322 cross-platform apps out of 10,000 popular apps for iOS and Android, respectively. We evaluate existing and new approaches for cross-platform app matching against a set of reference pairs that we obtained from Google’s data migration service. We identify a combination of seven features from app store metadata and the apps themselves to match iOS and Android apps with high confidence (95.82 %). Compared to previous attempts that identified 14 % of apps as cross-platform, we are able to match 34 % of apps in our dataset. To foster future research in the cross-platform analysis of mobile apps, we make our pipeline available to the community.CCS CONCEPTS• Software and its engineering → Software libraries and repositories; • General and reference → Empirical studies.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644896,Vienna Science and Technology Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555783,iOS;Android;mobile apps;app retrieval;app matching;app stores,Software libraries;Publishing;Operating systems;Scalability;Pipelines;Metadata;Software,,1.0,,71.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Keep Me Updated: An Empirical Study on Embedded JavaScript Engines in Android Apps,E. Wen; J. Zhou; X. Luo; G. Russello; J. Dietrich,"The University of Auckland, Auckland, New Zealand; Hong Kong Polytechnic University, Hong Kong; Hong Kong Polytechnic University, Hong Kong; The University of Auckland, Auckland, New Zealand; Victoria University of Wellington, Wellington, New Zealand",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,361,372,"Although JavaScript (JS) has been widely used in mobile development, little is known about the security implications of utilizing JS engines shipped as native app libraries. In this paper, we conduct an empirical study by designing a $\mathfrak{J}S{\text{ - }}Inspector$ pipeline to identify the embedded JS engines in Android apps and assess their security. We investigate over 65,000 Android apps released between Jan 2018 and July 2023. The results show that many popular apps use embedded JS engines, and their engines remain outdated for extended periods. Moreover, approximately 85% of apps have not received updates since their initial release. As such, over 70% of the identified embedded engines are vulnerable to known exploits. We further present case studies of popular apps catering to millions of users. By exploiting their unpatched JS engines through various strategies, such as man-in-the-middle attacks, intent abuse, and malicious mini-apps, we can easily seize control of the targeted apps and execute arbitrary code. This work highlights critical security concerns associated with embedded JS engines. It emphasizes the urgency for timely updates and enhanced security measures during app development.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644901,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555696,Android;JavaScript;Vulnerabilities;Hybrid,Codes;Pipelines;Libraries;Software;Security;Software measurement;Data mining,,1.0,,68.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Large Language Model vs. Stack Overflow in Addressing Android Permission Related Challenges,S. J. Oishwee; N. Stakhanova; Z. Codabux,"University of Saskatchewan, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,373,383,"The Android permission system regulates access to sensitive mobile device resources such as camera and location. To access these resources, third-party developers need to request permissions. However, the Android permission system is complex and fast-evolving, presenting developers with numerous challenges surrounding compatibility issues, misuse of permissions, and vulnerabilities related to permissions. Our study aims to explore whether Large Language Models (LLMs) can serve as a reliable tool to assist developers in using Android permissions correctly and securely, thereby reducing the risks of misuse and security vulnerabilities in apps. In our study, we analyzed 1,008 Stack Overflow questions related to Android permissions and their accepted answers. In parallel, we generate answers to these questions using a popular LLM tool, ChatGPT. We focused on how well the ChatGPT’s responses align with the accepted answers on Stack Overflow. Our findings show that above 50% of ChatGPT’s answers align with Stack Overflow’s accepted answers. ChatGPT offers better-aligned responses for challenges related to Documentation and Conceptual Understanding, while it provides less aligned answers for Debugging-related issues. In addition, we found that ChatGPT provides more consistent answers for 73.27% questions. Our study demonstrates the potential for using LLMs such as ChatGPT as a supporting tool to help developers navigate Android permission-related problems.CCS CONCEPTS• Security and privacy → Software and application security; • Computing methodologies → Artificial intelligence.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644933,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555796,Android Permissions;Stack Overflow;Large Language Model (LLM),Privacy;Navigation;Documentation;Chatbots;Software;Mobile handsets;Cognition,,1.0,,48.0,,18 Jun 2024,,,IEEE,IEEE Conferences
DATAR: A Dataset for Tracking App Releases,Y. Abedini; M. H. Hajihosseini; A. Heydarnoori,"Department of Computer Engineering, Sharif University of Technology; Department of Computer Engineering, Sharif University of Technology; Department of Computer Engineering, Sharif University of Technology",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,384,388,"Android apps continuously evolve to meet user expectations and thrive in the competitive environment of app stores. Hence, making informed decisions is crucial for the success of upcoming releases. In recent years, researchers have sought to aid developers in release planning by studying, analyzing, and modeling evolutionary information derived from tracking releases of various apps. They have demonstrated how the types of information provided on different platforms can effectively predict and assess the quality or success of an app. Nevertheless, this field needs a comprehensive dataset containing evolutionary information that tracks various releases of open-source Android apps. Existing datasets lack release-level information, are not publicly available, or do not cover a wide range of up-to-date apps. This paper introduces a dataset comprising diverse metadata adapted from GitHub and Google Play, along with impactful metrics extracted from the source code of 8,041 published releases of 1,363 open-source Android apps.CCS CONCEPTS• Software and its engineering → Software libraries and repositories.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644892,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555728,Android;Open-Source;DATAR;App Releases Dataset,Measurement;Analytical models;Software libraries;Source coding;Metadata;Software;Planning,,,,22.0,,18 Jun 2024,,,IEEE,IEEE Conferences
AndroZoo: A Retrospective with a Glimpse into the Future,M. Alecci; P. J. Ruiz Jiménez; K. Allix; T. F. Bissyandé; J. Klein,"SnT, University of Luxembourg, Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg, Luxembourg; Independent Researcher, Rennes, France; SnT, University of Luxembourg, Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg, Luxembourg",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,389,393,"In 2016, we released AndroZoo, a continuously expanding dataset of Android applications that aggregates apps from various sources, including the official Google Play app market. As of today, AndroZoo contains approximately 24 million APK files, making it, to the best of our knowledge, the most extensive dataset of Android APKs accessible to the research community. Currently, an average of 500 000 APKs are downloaded every day, with our initial MSR paper counting more than 880 citations on Google Scholar.Over time, we have consistently expanded AndroZoo, adapting to app markets’ evolution and refining our collection process. Additionally, we have started collecting supplementary data that could be valuable for various Android-related research projects and that we are providing to users, such as app descriptions, upload dates, ratings, lists of permissions, and many other kinds of data.This paper begins with a retrospective analysis of AndroZoo, offering statistics on APK files, apps, users, and downloads. Then, we introduce the new data we are releasing to users, offering insights and examples of their usage.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644863,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555779,Android Applications;APK;Software Repository;Metadata,Aggregates;Refining;Software;Reproducibility of results;Internet;Data mining,,1.0,,21.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Whodunit: Classifying Code as Human Authored or GPT-4 generated- A case study on CodeChef problems,O. J. Idialu; N. S. Mathews; R. Maipradit; J. M. Atlee; M. Nagappan,University of Waterloo; University of Waterloo; University of Waterloo; University of Waterloo; University of Waterloo,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,394,406,"Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI-generated code as their work. Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4. Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier on the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644926,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555768,code stylometry;chatgpt;AI code;GPT-4 generated code;authorship profiling;software engineering,Codes;Focusing;Machine learning;Software;Robustness;Regulation;Task analysis,,3.0,,50.0,,18 Jun 2024,,,IEEE,IEEE Conferences
GIRT-Model: Automated Generation of Issue Report Templates,N. Nikeghbal; A. H. Kargaran; A. Heydarnoori,"Munich, Germany; Ludwig Maximilian University, Munich, Germany; Bowling Green State University, Bowling Green, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,407,418,"Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer’s instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model.In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user study in which participants wrote short IRTs with GIRT-Model. Our results show that the participants find GIRT-Model useful in the automated generation of templates. We hope that through the use of GIRT-Model, we can encourage more developers to adopt IRTs in their repositories. We publicly release our code, dataset, and model at https://github.com/ISE-Research/girt-model.CCS CONCEPTS• Software and its engineering → Software notations and tools; Software creation and management",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644906,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555882,Issue Template Generation;Issue Report Template;Issue Template;Bug Template;GitHub;Issue Tracker;Bug Report,Codes;Software;Meteors;Data mining;Software development management,,1.0,,59.0,,18 Jun 2024,,,IEEE,IEEE Conferences
MicroRec: Leveraging Large Language Models for Microservice Recommendation,A. S. Alsayed; H. Khanh Dam; C. Nguyen,"University of Wollongong, Wollongong, Australia; University of Wollongong, Wollongong, Australia; University of Wollongong, Wollongong, Australia",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,419,430,"The increasing adoption of microservices in software development requires effective recommendation systems that guide developers to relevant microservices. In this paper, we introduce MicroRec, a novel microservice recommender framework which leverages insights from Stack Overflow posts and the power of Large Language Models (LLMs). MicroRec utilizes a dual-encoder architecture that combines contrastive learning and semantic similarity learning, allowing us to achieve robust and accurate retrieval and ranking of relevant posts based on user queries. Using LLMs, MicroRec builds up a deep understanding of both user queries and microservices through the information they provide (e.g., README files and Dockerfiles). Our empirical evaluations demonstrate significant improvements brought by MicroRec over the existing methods across a variety of performance metrics including MRR, MAP, and precision@k. In addition, the results returned by MicroRec were fourteen times more accurate than those provided by the existing recommendation tool on the widely-used Docker Hub platform.CCS CONCEPTS• Computing methodologies → Semantic networks; Natural language processing; • Applied computing → Document searching; • Information systems → Recommender systems.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644916,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555678,Microservices;Recommendation System;Semantic Search;Large Language Models;Docker Hub,Measurement;Accuracy;Semantics;Microservice architectures;Software;Natural language processing;Data mining,,,,37.0,,18 Jun 2024,,,IEEE,IEEE Conferences
PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software,W. Jiang; J. Yasmin; J. Jones; N. Synovic; J. Kuo; N. Bielanski; Y. Tian; G. K. Thiruvathukal; J. C. Davis,"Purdue University, W Lafayette, IN, USA; Queen’s University, Kingston, CA; Purdue University, W Lafayette, IN, USA; Purdue University, W Lafayette, IN, USA; Purdue University, W Lafayette, IN, USA; Purdue University, W Lafayette, IN, USA; Queen’s University, Kingston, CA; Loyola University Chicago, Chicago, IL, USA; Purdue University, W Lafayette, IN, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,431,443,"The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset’s comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model’s training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.Our artifact is available at https://github.com/PurdueDualityLab/PeaTMOSS-Artifact. Our dataset is available at https://transfer.rcac.purdue.edu/file-manager?origin_id=ff978999-16c2-4b50-ac7a-947ffdc3eb1d&origin_path=%2F.CCS Concepts• Computing methodologies → Artificial intelligence; Information extraction; • Information systems → Database design and models; • Software and its engineering → Software libraries and repositories.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644907,Google; Argonne National Laboratory; Natural Sciences and Engineering Research Council of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555612,Datasets;Machine learning;Deep neural networks;Model zoos;Package registries;Open-source;Empirical software engineering,Training;Analytical models;Software libraries;Computational modeling;Supply chains;Metadata;Licenses,,3.0,,112.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Data Augmentation for Supervised Code Translation Learning,B. Chen; J. Golebiowski; Z. Abedjan,"TU Berlin, Berlin, Germany; Amazon AWS, Berlin, Germany; Research Center, Leibniz Universität Hannover & L3S, Hannover, Germany",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,444,456,"Data-driven program translation has been recently the focus of several lines of research. A common and robust strategy is supervised learning. However, there is typically a lack of parallel training data, i.e., pairs of code snippets in the source and target language. While many data augmentation techniques exist in the domain of natural language processing, they cannot be easily adapted to tackle code translation due to the unique restrictions of programming languages. In this paper, we develop a novel rule-based augmentation approach tailored for code translation data, and a novel retrieval-based approach that combines code samples from unorganized big code repositories to obtain new training data. Both approaches are language-independent. We perform an extensive empirical evaluation on existing Java-C#-benchmarks showing that our method improves the accuracy of state-of-the-art supervised translation techniques by up to 35%.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644923,Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555825,,Training;Adaptation models;Computer languages;Codes;Accuracy;Supervised learning;Training data,,1.0,,51.0,,18 Jun 2024,,,IEEE,IEEE Conferences
On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study,A. M. Mir; M. Keshani; S. Proksch,"Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,457,468,"Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both with and without pruning. We find that CG pruning is a difficult task for real-world Java projects and substantial improvements in the CG precision (+25%) meet reduced recall (-9%). However, our experiments show promising results: even when we favor recall over precision by using an F2 metric in our experiments, we can show that pruned CGs have comparable quality to a context-sensitive 1-CFA analysis while being computationally less demanding. Resulting CGs are much smaller (69%), and substantially faster (3.5x speed-up), with virtually unchanged results in our downstream analysis.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644897,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555593,call graphs;machine learning;pruning;software analysis;empirical study,Training;Measurement;Java;Heuristic algorithms;Training data;Machine learning;Software,,,,62.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Leveraging GPT-like LLMs to Automate Issue Labeling,G. Colavito; F. Lanubile; N. Novielli; L. Quaranta,"University of Bari, Italy; University of Bari, Italy; University of Bari, Italy; University of Bari, Italy",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,469,480,"Issue labeling is a crucial task for the effective management of software projects. To date, several approaches have been put forth for the automatic assignment of labels to issue reports. In particular, supervised approaches based on the fine-tuning of BERT-like language models have been proposed, achieving state-of-the-art performance. More recently, decoder-only models such as GPT have become prominent in SE research due to their surprising capabilities to achieve state-of-the-art performance even for tasks they have not been trained for. To the best of our knowledge, GPT-like models have not been applied yet to the problem of issue classification, despite the promising results achieved for many other software engineering tasks. In this paper, we investigate to what extent we can leverage GPT-like LLMs to automate the issue labeling task. Our results demonstrate the ability of GPT-like models to correctly classify issue reports in the absence of labeled data that would be required to fine-tune BERT-like LLMs.CCS CONCEPTS• Software and its engineering → Documentation; Software evolution; Maintaining software; • Information systems → Clustering and classification;",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644903,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555675,LLM;Issue Labeling;GPT;Software Maintenance and Evolution;Labeling Unstructured Data,Annotations;Computational modeling;Scalability;Supervised learning;Manuals;Software;Data models,,4.0,,89.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Exploring the Effect of Multiple Natural Languages on Code Suggestion Using GitHub Copilot,K. Koyanagi; D. Wang; K. Noguchi; M. Kondo; A. Serebrenik; Y. Kamei; N. Ubayashi,"Kyushu University, Japan; Kyushu University, Japan; Kyushu University, Japan; Kyushu University, Japan; Eindhoven University of Technology, The Netherlands; Kyushu University, Japan; Kyushu University, Japan",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,481,486,"GitHub Copilot is an AI-enabled tool that automates program synthesis. It has gained significant attention since its launch in 2021. Recent studies have extensively examined Copilot’s capabilities in various programming tasks, as well as its security issues. However, little is known about the effect of different natural languages on code suggestion. Natural language is considered a social bias in the field of NLP, and this bias could impact the diversity of software engineering. To address this gap, we conducted an empirical study to investigate the effect of three popular natural languages (Eng-lish, Japanese, and Chinese) on Copilot. We used 756 questions of varying difficulty levels from AtCoder contests for evaluation purposes. The results highlight that the capability varies across natural languages, with Chinese achieving the worst performance. Furthermore, regardless of the type of natural language, the performance decreases significantly as the difficulty of questions increases. Our work represents the initial step in comprehending the significance of natural languages in Copilot’s capability and introduces promising opportunities for future endeavors.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644917,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555627,Code Suggestion;GitHub Copilot;Empirical Study,Codes;Natural languages;Programming;Software;Security;Data mining;Cultural differences,,2.0,,28.0,,18 Jun 2024,,,IEEE,IEEE Conferences
A Four-Dimension Gold Standard Dataset for Opinion Mining in Software Engineering,M. R. Islam; M. F. Rabbi; Y. Jo; A. Champa; E. Young; C. Wilson; G. Scott; M. Zibran,Lamar University; Idaho State University; Lamar University; Idaho State University; Lamar University; Lamar University; Lamar University; Idaho State University,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,487,491,"We present the first four-dimension gold standard dataset to advance opinion mining focused on the software engineering domain. Through a well-defined sampling and annotation strategy leveraging multiple coders, we construct a corpus of 2,000 Stack Overflow posts labeled with four dimensions/tuples, including sentiments, polar facts, aspects, and named entities. This multidimensional ground truth dataset opens up new research opportunities for opinion mining in domain-adapted NLP tools for software engineering by capturing existing relationships between extracted elements at a more granular level. It also facilitates investigating the effects of sentiments in the developers’ social forums.CCS CONCEPTS• Software and its engineering → Programming teams.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644893,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555833,Sentiment Analysis;Opinion Mining;Aspects;Named Entity;Software Engineering;Natural Language Processing,Sentiment analysis;Annotations;Collaboration;Programming;Benchmark testing;Software;Data mining,,1.0,,26.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Opening the Valve on Pure-Data: Usage Patterns and Programming Practices of a Data-Flow Based Visual Programming Language,A. Islam; K. Eng; A. Hindle,"University of Alberta, Edmonton, Canada; University of Alberta, Edmonton, Canada; University of Alberta, Edmonton, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,492,497,"Pure Data (PD), a data-flow based visual programming language utilized for music and sound synthesis, remains underexplored in software engineering research. Existing literature fails to address the nuanced programming practices within PD, prompting the need to investigate how end-users manipulate nodes and edges in this visual language. This paper systematically extracts and analyzes 6,534 publicly available PD projects from GitHub. Employing source code parsing, pattern matching, and statistical analysis, we unveil usage patterns of PD by the end-user programmers. We found that most revisions of the PD files are small and simple, with fewer than 64 nodes, 51 connections, and 3 revisions. Most PD projects have less than 17 PD files, 31 commits, and only 1 author working on the PD files. The median differences in the number of nodes and edges between each commit and its parents, modifying the same file, are 3 and 0, respectively, implying small changes across various revisions of a PD file. Our findings contribute a valuable dataset for future studies, addressing the dearth of research in PD. By unraveling usage patterns, we provide insights that empower scholars and practitioners to optimize the programming experience for end-users in the realm of visual programming languages.CCS CONCEPTS• Software and its engineering → Visual languages.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644865,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555832,Visual Programming Language;Pure Data;End-User Programmers,Visualization;Computer languages;Source coding;Collaboration;Programming;Metadata;Software,,3.0,,21.0,,18 Jun 2024,,,IEEE,IEEE Conferences
The PIPr Dataset of Public Infrastructure as Code Programs,D. Sokolowski; D. Spielmann; G. Salvaneschi,"University of St. Gallen, Switzerland; University of St. Gallen, Switzerland; University of St. Gallen, Switzerland",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,498,503,"With Programming Languages Infrastructure as Code (PL-IaC), developers implement IaC programs in popular imperative programming languages like Python and Typescript. Such programs generate the declarative target state of the deployment, i.e., they describe what to set up, not how to set it up. Despite the popularity of PL-IaC, which has grown more than ten times from 2020 to 2023, we know little about how developers apply it and how IaC programs differ from other software. Such knowledge is essential to effectively use existing software engineering techniques and develop new ones for PL-IaC. To shed light on PL-IaC in practice, we present PIPr, the first systematic PL-IaC dataset. PIPr is based on 37 712 public IaC programs on GitHub from August 2022 and includes initial analyses, assessing the programming languages, testing techniques, and licenses of the IaC programs. Beyond the metadata and analysis results of all IaC programs, PIPr contains the code of all 15 504 IaC programs whose licenses permit redistribution. PIPr sets the ground for future in-depth investigations on PL-IaC in practice.CCS CONCEPTS•Software and its engineering Architecture description languages; Software configuration→ management and version control systems; Cloud computing.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644888,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555773,Infrastructure as Code;Pulumi;AWS CDK;CDKTF;Testing,Codes;Systematics;Source coding;Licenses;Metadata;Software;Testing,,,,47.0,,18 Jun 2024,,,IEEE,IEEE Conferences
A Dataset of Microservices-based Open-Source Projects,D. A. d’Aragona; A. Bakhtin; X. Li; R. Su; L. Adams; E. Aponte; F. Boyle; P. Boyle; R. Koerner; J. Lee; F. Tian; Y. Wang; J. Nyyssölä; E. Quevedo; S. M. Rahaman; A. S. Abdelfattah; M. Mäntylä; T. Cerny; D. Taibi,"Tampere University, Tampere, Finland; University of Oulu, Oulu, Finland; University of Oulu, Oulu, Finland; University of Oulu, Oulu, Finland; Baylor University, USA; Universidad del Sagrado Corazón, USA; Baylor University, USA; Baylor University, USA; Baylor University, USA; University of Richmond, USA; University of Oulu, Oulu, Finland; University of Helsinki, Helsinki, Finland; University of Helsinki, Helsinki, Finland; Baylor University, USA; Baylor University, USA; Baylor University, USA; University of Helsinki, Oulu, Finland; SIE, University of Arizona, USA; University of Oulu, Oulu, Finland",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,504,509,"Researchers in the microservices community often resort to demonstrating the impact of their proposed advancements on custom- made microservices projects. This is a possible source of bias that can reduce the trustworthiness of the results. Moreover, it is hard to compare advances in small projects, often developed due to lack of time. It is common across disciplines to recognize benchmarks that mitigate bias and unify the advancements’ impact. To facilitate the identification of available open-source microservice projects (OSS-MS), we performed a comprehensive study to identify, curate, and catalog OSS-MS. We started with 389559 projects and filtered them down to 3804 projects that we manually labeled. After manual labeling, our dataset contains 378 projects with three or more microservices and with over 100 commits. We document the projects from many perspectives, including project size, platform, number of contributors, project purpose, and foundation support. This dataset can serve researchers as a roadmap to identify benchmarks, as our dataset can be used to answer questions such as whether the number of services impacts the issue count.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644890,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555704,,Industries;Pipelines;Microservice architectures;Manuals;Computer architecture;Benchmark testing;Software systems,,5.0,,14.0,,18 Jun 2024,,,IEEE,IEEE Conferences
SensoDat: Simulation-based Sensor Dataset of Self-driving Cars,C. Birchler; C. Rohrbach; T. Kehrer; S. Panichella,"Zurich University of Applied Sciences, University of Bern, Switzerland; University of Bern, Switzerland; University of Bern, Switzerland; Zurich University of Applied Sciences, Switzerland",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,510,514,"Developing tools in the context of autonomous systems [22], [24], such as self-driving cars (SDCs), is time-consuming and costly since researchers and practitioners rely on expensive computing hardware and simulation software. We propose SensoDat, a dataset of 32,580 executed simulation-based SDC test cases generated with state-of-the-art test generators for SDCs. The dataset consists of trajectory logs and a variety of sensor data from the SDCs (e.g., rpm, wheel speed, brake thermals, transmission, etc.) represented as a time series. In total, SensoDat provides data from 81 different simulated sensors. Future research in the domain of SDCs does not necessarily depend on executing expensive test cases when using SensoDat. Furthermore, with the high amount and variety of sensor data, we think SensoDat can contribute to research, particularly for AI development, regression testing techniques for simulation-based SDC testing, flakiness in simulation, etc. Link to the dataset: https://doi.org/10.5281/zenodo.10307479",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644891,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555662,,Time series analysis;Wheels;Thermal sensors;Software;Hardware;Generators;Autonomous automobiles,,,,29.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Incivility in Open Source Projects: A Comprehensive Annotated Dataset of Locked GitHub Issue Threads,R. Ehsani; M. M. Imran; R. Zita; K. Damevski; P. Chatterjee,"Drexel University, Philadelphia, PA, USA; Virginia Commonwealth University, Richmond, VA, USA; Elmhurst University, Elmhurst, IL, USA; Virginia Commonwealth University, Richmond, VA, USA; Drexel University, Philadelphia, PA, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,515,519,"In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations. This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects. We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility. We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset. The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively. This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior.CCS CONCEPTS•Software and its engineering → Software organization and properties.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644887,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555803,OSS;GitHub;developer conversations;incivility,Collaboration;Organizations;Data mining;Open source software;Message systems;Software development management,,,,44.0,,18 Jun 2024,,,IEEE,IEEE Conferences
A Dataset of Atoms of Confusion in the Android Open Source Project,D. Tabosa; O. Pinheiro; L. Rocha; W. Viana,"Federal University of Ceará, Fortaleza, Ceará, BR; Federal University of Ceará, Fortaleza, Ceará, BR; Federal University of Ceará, Fortaleza, Ceará, BR; Federal University of Ceará, Fortaleza, Ceará, BR",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,520,524,"Ensuring the readability and comprehension of source code is key for effective software maintenance and evolution, particularly in tasks involving bug fixing, refactoring, and optimization. Previous studies highlight that challenges in maintenance and the emergence of certain bugs can be traced back to small code fragments called ""Atoms of Confusion"" (AC). While initial investigations identified these snippets in C++ code bases, subsequent studies have identified analogous structures in languages such as Java. Although numerous studies have delved into observing ACs in Java projects, there is a lack of studies focused on the Android ecosystem. This paper aims to address this gap by constructing a comprehensive dataset, which catalogs ACs and assesses their prevalence in the Android Open Source Project (AOSP). After analyzing over 125,000 source code files across 370 Git repositories, our findings reveal that more than 30% of Java files within the AOSP contain at least one AC, totaling over 320,000 recorded instances. This equates to one AC for approximately every 91 lines of code. Particularly, this dataset can be used to support further studies on shedding light on the ACs prevalence in the Android ecosystem, alongside their relation with traditional object-oriented software metrics (e.g., size, complexity, cohesion, and coupling), also recorded in the dataset. Using this dataset, researchers may provide valuable insights for developers and software engineers, emphasizing the need for strategies to mitigate the possible impact of ACs on software maintenance.CCS CONCEPTS•General and reference → Empirical studies; • Software and its engineering → Maintaining software; Software defect analysis; • Human-centered computing → Collaborative interaction.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644874,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555761,Android;Atoms of Confusion;AOSP,Java;Software maintenance;Codes;Source coding;Ecosystems;Computer bugs;Atoms,,,,16.0,,18 Jun 2024,,,IEEE,IEEE Conferences
PlayMyData: a curated dataset of multi-platform video games,A. D’Angelo; C. Di Sipio; C. Politowski; R. Rubei,"Università Degli Studi Dell’Aquila, L’Aquila, Italy; Università Degli Studi Dell’Aquila, L’Aquila, Italy; DIRO, University of Montreal, Montreal, Canada; Università Degli Studi Dell’Aquila, L’Aquila, Italy",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,525,529,"Being predominant in digital entertainment for decades, video games have been recognized as valuable software artifacts by the software engineering (SE) community just recently. Such an acknowledgment has unveiled several research opportunities, spanning from empirical studies to the application of AI techniques for classification tasks. In this respect, several curated game datasets have been disclosed for research purposes even though the collected data are insufficient to support the application of advanced models or to enable interdisciplinary studies. Moreover, the majority of those are limited to PC games, thus excluding notorious gaming platforms, e.g., PlayStation, Xbox, and Nintendo. In this paper, we propose PlayMyData, a curated dataset composed of 99,864 multi-platform games gathered by the IGDB website. By exploiting a dedicated API, we collect relevant metadata for each game, e.g., description, genre, rating, gameplay video URLs, and screenshots. Furthermore, we enrich PlayMyData with the timing needed to complete each game by mining the HLTB website. To the best of our knowledge, this is the most comprehensive dataset in the domain that can be used to support different automated tasks in SE. More importantly, PlayMyData can be used to foster cross-domain investigations built on top of the provided multimedia data.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644869,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555668,video games;data mining;software engineering,Video games;Entertainment industry;Games;Streaming media;Metadata;Software;Timing,,,,35.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Questioning the Questions We Ask About the Impact of AI on Software Engineering : MSR 2024 Keynote,M. -A. Storey,"University of Victoria, Victoria, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,530,530,"The recent advent and wide diffusion of generative AI has initiated a fundamental change in how software is developed. This technology is just one innovation along a long arc of disruptions in software engineering that include the internet, high-level programming languages, integrated development environments, open source, agile development, and social coding environments. Disruptive technologies such as these show the potential to augment and accelerate development activities along many socio-technical dimensions, while altering fundamental business processes and paradigms. Yet paradoxically, these innovations have the potential to eventually undermine the very advancements they seek to promote, rendering technologies and methods obsolete [1].When any new disruptive technology emerges, successful software companies that traditionally respond well to incremental innovations often fail when they suffer from inertia to change or don’t anticipate how people will interact with the new technology. Similarly, researchers constrained by rigid research discipline can be slow to react, and may fail to recognize important and urgent societal and industrial needs. Researchers and companies alike may struggle in knowing which metrics to use and even how to measure the impact of change, further misleading their efforts to adapt.In this talk, I question the way we select research questions in software engineering and how we study them, particularly in the face of innovations such as generative AI. To provoke a change in our research, I introduce a disruptive playbook to steer us towards broader and more novel research directions. This step-by-step playbook is first illustrated by applying it to a prior disruptive technology, Stack Overflow. I will discuss how the playbook provides a new lens for reflecting on this body of research and how doing so reveals new insights. I then use the playbook, assisted with a customized research playbook GPT, to brainstorm and frame new research directions about the emerging disruptive innovations in software engineering that are being built on top of generative AI.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644895,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555819,Software Engineering;Disruptive Innovations;Playbook;Research Questions,Technological innovation;Generative AI;Companies;Rendering (computer graphics);Software;Software measurement;Disruptive technologies,,,,2.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Learning to Predict and Improve Build Successes in Package Ecosystems,H. Menon; D. Nichols; A. Bhatele; T. Gamblin,"Lawrence Livermore National Laboratory, Livermore, California, USA; University of Maryland, College Park, Maryland, USA; University of Maryland, College Park, Maryland, USA; Lawrence Livermore National Laboratory, Livermore, California, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,531,542,"Software has become increasingly complex, with a typical application depending on tens or hundreds of packages. Finding compatible versions and build configurations of these packages is challenging. This paper presents a method to learn the likelihood of software build success, and techniques for leveraging this information to guide dependency solvers to better software configurations. We leverage the heavily parameterized package recipes from the Spack package manager to produce a training data set of builds, and we use Graph Neural Networks to learn whether a given package configuration will build successfully or not. We apply our tool to the U.S. Exascale Computing Project’s software stack. We demonstrate its effectiveness in predicting whether a given package will build successfully. We show that our technique can be used to improve the solutions generated by dependency solvers, reducing the need for developers to find working builds by trial and error.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644927,Lawrence Livermore National Laboratory; Laboratory Directed Research and Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555617,,Accuracy;Ecosystems;Training data;Full stack;Production;Predictive models;Software,,1.0,,45.0,,18 Jun 2024,,,IEEE,IEEE Conferences
The Impact of Code Ownership of DevOps Artefacts on the Outcome of DevOps CI Builds,A. Kola-Olawuyi; N. R. Weeraddana; M. Nagappan,University of Waterloo; University of Waterloo; University of Waterloo,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,543,555,"DevOps is a key element in sustaining the quality and efficiency of software development. Yet, the effectiveness of DevOps methodologies extends beyond just technological expertise. It is greatly affected by the manner in which teams handle and engage with DevOps artefacts. Grasping the intricacies of code ownership and contribution patterns within DevOps artefacts is vital for refining strategies and ensuring they deliver their full potential.There are two main strategies to manage DevOps artefacts as suggested in prior work: (1) all project developers need to contribute to DevOps artefacts, and (2) a dedicated group of developers needs to be authoring DevOps artefacts. To analyze which strategy works best for Open-Source Software (OSS) projects, we conduct an empirical analysis on a dataset of 892,193 CircleCI builds spanning 1,689 OSS projects. We employ a two-pronged approach to our study. First, we investigate the impact of chronological code ownership of DevOps artefacts on the outcome of a CI build on a build level. Second, we study the impact of the Skewness of DevOps contributions on the success rate of CI builds at the project level.Our findings reveal that, in general, larger chronological ownership and higher Skewness values of DevOps contributions are related to more successful build outcomes and higher rates of successful build outcomes, respectively. We further find that projects with low Skewness values could have high build success rates when the number of developers in the project is relatively small. Thus, our results suggest that while larger software organizations are better off having dedicated DevOps developers, smaller organizations would benefit from having all developers involved in DevOps.CCS CONCEPTS•Do Not Use This Code → Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644924,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555822,DevOps;Code Ownership;Continuous Integrations;Empirical Study.,DevOps;Codes;Refining;Collaboration;Organizations;Grasping;Predictive models,,,,73.0,,18 Jun 2024,,,IEEE,IEEE Conferences
A Mutation-Guided Assessment of Acceleration Approaches for Continuous Integration: An Empirical Study of YourBase,Z. Zeng; T. Xiao; M. Lamothe; H. Hata; S. McIntosh,"Software REBELs, University of Waterloo, Canada; Division of Information Science, Nara Institute of Science and Technology, Japan; Department of Computer Engineering and Software Engineering, Polytechnique Montréal, Canada; Faculty of Engineering, Shinshu University, Japan; Software REBELs, University of Waterloo, Canada",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,556,568,"Continuous Integration (CI) is a popular software development practice that quickly verifies updates to codebases. To cope with the ever-increasing demand for faster software releases, CI acceleration approaches have been proposed; however, adoption of CI acceleration is not without risks. For example, CI acceleration products may mislabel change sets (e.g., a build labeled as failing that passes in an unaccelerated setting or vice versa) or produce results that are inconsistent with an unaccelerated build (e.g., the underlying reasons for failure differ between (un)accelerated builds). These inconsistencies threaten the trustworthiness of CI acceleration products.In this paper, we propose an approach inspired by mutation testing to systematically evaluate the trustworthiness of CI acceleration. We apply our approach to YourBase, a program analysis-based CI acceleration product, and uncover issues that hinder its trustworthiness. First, we study how often the same build in accelerated and unaccelerated CI settings produce different mutation testing outcomes. We call mutants with different outcomes in the two settings ""gap mutants"". Next, we study the code locations where gap mutants appear. Finally, we inspect gap mutants to understand why acceleration causes them to survive. Our analysis of ten open-source projects uncovers 2,237 gap mutants. We find that: (1) the gap mutants account for 0.11%–23.50% of the studied mutants; (2) 88.95% of gap mutants can be mapped to specific source code functions and classes using the dependency representation of the studied CI acceleration product; and (3) 69% of gap mutants survive CI acceleration due to deterministic reasons that can be classified into six fault patterns. Our results show that even deterministic CI acceleration solutions suffer from trustworthiness limitations, and highlight the ways in which trustworthiness could be pragmatically improved.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644914,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555653,CI Acceleration;CI Trustworthiness;Mutation Testing,Accelerometers;Codes;Source coding;Life estimation;Software;Complexity theory;Reliability,,,,81.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Cohort Studies for Mining Software Repositories,N. Saarimäki; S. Vegas; V. Lenarduzzi; D. Taibi; M. Robredo,University of Luxembourg; Universidad Politécnica de Madrid; University of Oulu; University of Oulu; University of Oulu,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,569,570,"Mining Software Repositories studies have become increasingly popular over the years. However, a notable limitation is that they report correlational relationships rather than establishing causation. In contrast, certain disciplines (e.g. epidemiology) have developed specific methods to address this limitation. The goal of this tutorial is to introduce participants to one such method: cohort studies. By the end of the tutorial, participants will be familiar with the steps and techniques involved in designing and analyzing cohort studies.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3649103,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555741,Cohort Study;Empirical Software Engineering;MSR,Tutorials;Software;Data mining;Epidemiology,,,,9.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Unveiling ChatGPT’s Usage in Open Source Projects: A Mining-based Study,R. Tufano; A. Mastropaolo; F. Pepe; O. Dabić; M. Di Penta; G. Bavota,"SEART @ Software Institute, Università Della Svizzera Italiana, Switzerland; SEART @ Software Institute, Università Della Svizzera Italiana, Switzerland; Department of Engineering, University of Sannio, Italy; SEART @ Software Institute, Università Della Svizzera Italiana, Switzerland; SEART @ Software Institute, Università Della Svizzera Italiana, Switzerland; Department of Engineering, University of Sannio, Italy",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,571,583,"Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI’s ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.CCS CONCEPTS•Software and its engineering → Software tools.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644918,European Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555619,ChatGPT;Empirical study,Automation;Taxonomy;Chatbots;Filling;Encoding;Data mining;Task analysis,,2.0,,118.0,,18 Jun 2024,,,IEEE,IEEE Conferences
DRMiner: A Tool For Identifying And Analyzing Refactorings In Dockerfile,E. Ksontini; A. Abid; R. Khalsi; M. Kessentini,"University of Michigan - Flint, USA; Oakland University, USA; University of Michigan - Flint, USA; University of Michigan - Flint, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,584,594,"Software containerization using Docker has recently become the de facto standard for delivering reusable software artifacts. Integral to Docker’s functionality are Dockerfiles, which serve as scripts that define the layers and components to be incorporated within a container. Although these files serve as the bedrock of container creation, their maintenance presents intricate challenges. Specifically, the task of Dockerfile refactoring is compounded by its inherent complexity. Although the importance of refactoring inside Docker ecosystems is apparent, detecting it remains challenging. Developers usually avoid documenting their refactoring efforts, often combining them with other changes.While previous research works have delved into Docker refactoring, the predominant focus has been on empirical foundations, resulting in a constrained and narrow viewpoint. Despite all endeavors, there remains a clear gap for an exhaustive tool that can adeptly navigate the complexities of Dockerfile refactoring detection. To fill this gap, we introduce DRMiner, the first tool for identifying and analyzing refactoring in Dockerfile. Our solution, designed, implemented, and evaluated in terms of correctness and generalization, relies on a novel E-AST(Enhanced Abstract Syntax Tree) based component-matching algorithm and a set of detection rules to determine refactoring candidates. This work will serve as a fundamental building block for the refactoring detection in the realm of Docker.CCS CONCEPTS• Software and its engineering → Software maintenance tools.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644921,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555740,Docker;Dockerfiles;AST;Refactoring;Commit;Git,Software maintenance;Reviews;Navigation;Software algorithms;Containers;Syntactics;Maintenance,,1.0,,40.0,,18 Jun 2024,,,IEEE,IEEE Conferences
A Large-Scale Empirical Study of Open Source License Usage: Practices and Challenges,J. Wu; L. Bao; X. Yang; X. Xia; X. Hu,"The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Huawei, China; The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,595,606,"The popularity of open source software (OSS) has led to a significant increase in the number of available licenses, each with their own set of terms and conditions. This proliferation of licenses has made it increasingly challenging for developers to select an appropriate license for their projects and to ensure that they are complying with the terms of those licenses. As a result, there is a need for empirical studies to identify current practices and challenges in license usage, both to help developers make informed decisions about license selection and to ensure that OSS is being used and distributed in a legal and ethical manner. Moreover, the development of new licenses might be required to better meet the needs of the open source community and address emerging legal issues.In this paper, we conduct a large-scale empirical study of license usage across five package management platforms, i.e., Maven, NPM, PyPI, RubyGems, and Cargo. Our objective is to examine the current trends and potential issues in license usage of the OSS community. In total, we analyze the licenses of 33,710,877 packages across the selected five platforms. We statistically analyze licenses in package management platforms from multiple perspectives, e.g., license usage, license incompatibility, license updates, and license evolution. Moreover, we conduct a comparative study of various aspects of core packages and common packages in these platforms. Our results reveal irregularities in license names and license incompatibilities that require attention. We observe both similarities and differences in license usage across the five platforms, with Cargo being the most standardized among them. Finally, we discuss some implications for actions based on our findings.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644900,National Key Research and Development Program of China; National Science Foundation; Fundamental Research Funds for the Central Universities; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555724,OSS Licenses;Empirical Study;Package Management Platform,Ethics;Law;Licenses;Metadata;Market research;Data mining;Open source software,,2.0,,51.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Analyzing the Evolution and Maintenance of ML Models on Hugging Face,J. Castaño; S. Martínez-Fernández; X. Franch; J. Bogner,Universitat Politècnica de Catalunya; Universitat Politècnica de Catalunya; Universitat Politècnica de Catalunya; Vrije Universiteit Amsterdam,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,607,618,"Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF – aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evolution across development stages of commits metrics and introduce a new classification system that estimates the maintenance status of models based on multiple attributes. This study aims to provide valuable insights about ML model maintenance and evolution that could inform future model development strategies on platforms like HF.CCS CONCEPTS•Information systems → Data mining; • Software and its engineering → Software maintenance tools; Software libraries and repositories.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644898,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555709,repository mining;software evolution;maintenance,Analytical models;Adaptation models;Technological innovation;Biological system modeling;Hafnium;Predictive models;Market research,,2.0,,45.0,,18 Jun 2024,,,IEEE,IEEE Conferences
On the Anatomy of Real-World R Code for Static Analysis,F. Sihler; L. Pietzschmann; R. Straub; M. Tichy; A. Diera; A. Dahou,"Ulm University, Germany; Ulm University, Germany; Ulm University, Germany; Ulm University, Germany; Ulm University, Germany; GESIS - Institute for the Social Sciences, Germany",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,619,630,"Context The R programming language has a huge and active community, especially in the area of statistical computing. Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs. At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice. Objective In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real- world R programs and packages to identify their characteristics and the features that are actually used. Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors. We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. Method We analyze 4 230 R scripts submitted alongside publications and the sources of 19 450 CRAN packages for over 350 000 R files, collecting and summarizing quantitative information for features of interest. Results We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions. Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions. Conclusion R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities. We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like loadCCS CONCEPTS•General and reference → Empirical studies; • Software and its engineering → Language features.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644911,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555880,R Programming Language;Large-Scale Static Analysis;Language Feature Usage,R language;Codes;Semantics;Static analysis;Programming;Software;Object recognition,,,,42.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Encoding Version History Context for Better Code Representation,H. Nguyen; C. Treud; P. Thongtanunam,"The University of Melbourne, Australia; Singapore Management University, Singapore; The University of Melbourne, Australia",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,631,636,"With the exponential growth of AI tools that generate source code, understanding software has become crucial. When developers comprehend a program, they may refer to additional contexts to look for information, e.g. program documentation or historical code versions. Therefore, we argue that encoding this additional contextual information could also benefit code representation for deep learning. Recent papers incorporate contextual data (e.g. call hierarchy) into vector representation to address program comprehension problems. This motivates further studies to explore additional contexts, such as version history, to enhance models’ understanding of programs. That is, insights from version history enable recognition of patterns in code evolution over time, recurring issues, and the effectiveness of past solutions. Our paper presents preliminary evidence of the potential benefit of encoding contextual information from the version history to predict code clones and perform code classification. We experiment with two representative deep learning models, ASTNN and CodeBERT, to investigate whether combining additional contexts with different aggregations may benefit downstream activities. The experimental result affirms the positive impact of combining version history into source code representation in all scenarios; however, to ensure the technique performs consistently, we need to conduct a holistic investigation on a larger code base using different combinations of contexts, aggregation, and models. Therefore, we propose a research agenda aimed at exploring various aspects of encoding additional context to improve code representation and its optimal utilisation in specific situations.CCS Concepts•Computing methodologies → Neural networks; • Software and its engineering → Reusability.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644929,Australian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555644,Source code representation;additional context;version history,Deep learning;Codes;Source coding;Neural networks;Documentation;Software;Vectors,,,,36.0,,18 Jun 2024,,,IEEE,IEEE Conferences
CodeLL: A Lifelong Learning Dataset to Support the Co-Evolution of Data and Language Models of Code,M. Weyssow; C. Di Sipio; D. Di Ruscio; H. Sahraoui,"DIRO, Université de Montréal; University of l’Aquila; University of l’Aquila; DIRO, Université de Montréal",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,637,641,"Motivated by recent work on lifelong learning applications for language models (LMs) of code, we introduce CodeLL, a lifelong learning dataset focused on code changes. Our contribution addresses a notable research gap marked by the absence of a long-term temporal dimension in existing code change datasets, limiting their suitability in lifelong learning scenarios. In contrast, our dataset aims to comprehensively capture code changes across the entire release history of open-source software repositories. In this work, we introduce an initial version of CodeLL, comprising 71 machine-learning-based projects mined from Software Heritage. This dataset enables the extraction and in-depth analysis of code changes spanning 2,483 releases at both the method and API levels. CodeLL enables researchers studying the behaviour of LMs in lifelong fine-tuning settings for learning code changes. Additionally, the dataset can help studying data distribution shifts within software repositories and the evolution of API usages over time.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644864,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555636,deep learning for code;lifelong learning;dataset,Deep learning;Codes;Limiting;Data models;History;Data mining;Open source software,,,,28.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Bidirectional Paper-Repository Tracing in Software Engineering,D. Garijo; M. Arroyo; E. Gonzalez; C. Treude; N. Tarocco,"Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; Universidad Politécnica de Madrid, Madrid, Spain; The University of Melbourne, Melbourne, Australia; CERN, Geneva, Switzerland",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,642,646,"While computer science papers frequently include their associated code repositories, establishing a clear link between papers and their corresponding implementations may be challenging due to the number of code repositories used in research publications. In this paper we describe a lightweight method for effectively identifying bidirectional links between papers and repositories from both LaTeX and PDF sources. We have used our approach to analyze more than 14000 PDF and Latex files in the Software Engineering category of Arxiv, generating a dataset of more than 1400 paper-code implementations and assessing current citation practices on it.CCS CONCEPTS•Applied computing → Document analysis; • Software and its engineering → Software libraries and repositories.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644876,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555881,Research software;article analysis;software citation;Open Science,Computer science;Codes;Text analysis;Software libraries;Portable document format;Software,,,,23.0,,18 Jun 2024,,,IEEE,IEEE Conferences
DistilKaggle: A Distilled Dataset of Kaggle Jupyter Notebooks,M. M. Ghahfarokhi; A. Asgari; M. Abolnejadian; A. Heydarnoori,"Department of Computer Engineering, Sharif University of Technology; Department of Computer Engineering, Sharif University of Technology; Department of Computer Engineering, Sharif University of Technology; Department of Computer Engineering, Sharif University of Technology",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,647,651,"Jupyter notebooks have become indispensable tools for data analysis and processing in various domains. However, despite their widespread use, there is a notable research gap in understanding and analyzing the contents and code metrics of these notebooks. This gap is primarily attributed to the absence of datasets that encompass both Jupyter notebooks and extracted their code metrics. To address this limitation, we introduce DistilKaggle, a unique dataset specifically curated to facilitate research on code metrics in Jupyter notebooks, utilizing the Kaggle repository as a prime source. Through an extensive study, we identify thirty-four code metrics that significantly impact Jupyter notebook code quality. These features such as lines of code cell, mean number of words in markdown cells, performance tier of developer, etc., are crucial for understanding and improving the overall effectiveness of computational notebooks. The DistilKaggle dataset which is derived from a vast collection of notebooks constitutes two distinct datasets: (i) Code Cells and Markdown Cells Dataset which is presen’ workflows as dataframes. It provides a granular view of the content structure within 542,051 Jupyter notebooks, enabling detailed analysis of code and markdown cells; and (ii) The Notebook Code Metrics Dataset focused on the identified code metrics of notebooks. Researchers can leverage this dataset to access Jupyter notebooks with specific code quality characteristics, surpassing the limitations of filters available on the Kaggle website. Furthermore, the reproducibility of the notebooks in our dataset is ensured through the code cells and markdown cells datasets, offering a reliable foundation for researchers to build upon. Given the substantial size of our datasets, it becomes an invaluable resource for the research community, surpassing the capabilities of individual Kaggle users to collect such extensive data. For accessibility and transparency, both the dataset and the code utilized in crafting this dataset are publicly available at https://github.com/ISE-Research/DistilKaggle.CCS CONCEPTS•Software and its engineering → Software libraries and repositories.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644882,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555812,Open dataset;Kaggle;Jupyter notebooks;Code metrics;Code quality,Measurement;Deep learning;Solid modeling;Codes;Software libraries;Filters;Feature extraction,,2.0,,34.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Estimating Usage Of Open Source Projects,S. Vargas; G. J. P. Link; J. Lee,"Google, New York, NY, USA; Bitergia, Omaha, NE, USA; Google, Mountain View, CA, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,652,653,"While open source projects can benefit from usage telemetry to understand their impact, privacy concerns often arise. This paper explores using publicly available metrics as proxies for actual usage data. Using the Flutter project as a case study, we found a strong correlation between these public metrics and Flutter’s active user count, demonstrating that such publicly available metrics can offer insights into project usage while respecting user privacy.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3645066,Google; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555650,Open Source Project Usage;Telemetry Data;Project Health Metrics,Measurement;Privacy;Correlation;Software;Telemetry;Data mining,,,,0.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Options Matter: Documenting and Fixing Non-Reproducible Builds in Highly-Configurable Systems,G. A. Randrianaina; D. Eddine Khelladi; O. Zendra; M. Acher,"CNRS, Inria, IRISA UMR 6074, Univ Rennes, Rennes, France; CNRS, Inria, IRISA UMR 6074, Univ Rennes, Rennes, France; CNRS, Inria, IRISA UMR 6074, Univ Rennes, Rennes, France; CNRS, Inria, IRISA Institut Universitaire de France (IUF) UMR 6074, Univ Rennes, Rennes, France",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,654,664,"A critical aspect of software development, build reproducibility, ensures the dependability, security, and maintainability of software systems. Although several factors, including the build environment, have been investigated in the context of non-reproducible builds, to the best of our knowledge the precise influence of configuration options in configurable systems has not been thoroughly investigated. This paper aims at filling this gap.This paper thus proposes an approach to automatically identify configuration options causing non-reproducibility of builds. It begins by building a set of builds in order to detect non-reproducible ones through binary comparison. We then develop automated techniques that combine statistical learning with symbolic reasoning to analyze over 20,000 configuration options. Our methods are designed to both detect options causing non-reproducibility, and remedy non-reproducible configurations, two tasks that are challenging and costly to perform manually.We evaluate our approach on three case studies, namely Toybox, Busybox, and Linux, analyzing more than 2,000 configurations for each of them. Toybox and Busybox come exempt from non-reproducibility. In contrast, 47% of Linux configurations lead to non-reproducible builds. The approach we propose in this paper is capable of identifying 10 configuration options that caused this non-reproducibility. When confronted to the Linux documentation, none of these are documented as non-reproducible. Thus, our identified non-reproducible configuration options are novel knowledge and constitute a direct, actionable information improvement for the Linux community. Finally, we demonstrate that our methodology effectively identifies a set of undesirable option values, enabling the enhancement and expansion of the Linux kernel documentation while automatically rectifying 96% of encountered non-reproducible builds.CCS CONCEPTS• Software and its engineering → Software configuration management and version control systems; Software system models.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644913,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555868,Reproducible Build;Build System;Highly-configurable System,Linux;Statistical learning;Documentation;Software systems;Cognition;Reproducibility of results;Security,,,,40.0,,18 Jun 2024,,,IEEE,IEEE Conferences
How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions,J. H. Bernardo; D. Alencar Da Costa; S. Q. de Medeiros; U. Kulesza,"Federal Institute of Rio Grande do Norte, Federal University of Rio Grande do Norte, Natal, Brazil; University of Otago, Dunedin, New Zealand; Federal University of Rio Grande do Norte, Natal, Brazil; Federal University of Rio Grande do Norte, Natal, Brazil",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,665,676,"Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build duration, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure. These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively.CCS CONCEPTS• Computing methodologies → Machine learning.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644915,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555764,continuous integration;machine learning;github actions;mining software repositories,Codes;Machine learning;Market research;Software;Data mining;Software development management;Testing,,1.0,,50.0,,18 Jun 2024,,,IEEE,IEEE Conferences
A dataset of GitHub Actions workflow histories,G. Cardoen; T. Mens; A. Decan,"University of Mons, Mons, Belgium; University of Mons, Mons, Belgium; University of Mons, Mons, Belgium",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,677,681,"GitHub Actions is the de facto workflow automation tool for GitHub repositories. Its popularity has increased dramatically over the recent years, opening up opportunities for empirical studies related to its usage. To enable such studies, we implemented gigawork, an open source tool for extracting the commit histories of changes to workflow files in GitHub repositories. Using this tool we collected and publicly released a dataset of 160K+ commit histories of workflow files in 32K+ public GitHub repositories, covering 1.5M+ workflow file versions. In order to facilitate its use by other researchers, the dataset includes relevant metadata related to workflow file changes in each commit. gigawork is publicly released on PyPi. Its associated dataset can be found on Zenodo (DOI: 10.5281/zenodo.10259013).",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644867,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555756,software repository mining;GitHub Actions;software evolution;workflow automation;empirical software engineering,Automation;Metadata;Software;History;Data mining;Usability;Task analysis,,3.0,,22.0,,18 Jun 2024,,,IEEE,IEEE Conferences
gawd: A Differencing Tool for GitHub Actions Workflows,P. R. Mazrae; A. Decan; T. Mens,"University of Mons (UMONS), Mons, Belgium; F.R.S.-FNRS Research Associate University of Mons (UMONS), Mons, Belgium; University of Mons (UMONS), Mons, Belgium",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,682,686,"The GitHub social coding platform introduced GitHub Actions as a way to automate different aspects of collaborative software development through the use of workflow files. It is the most popular CI/CD and workflow automation tool for GitHub. To maintain workflow code over time, it is useful to rely on differencing tools to identify the changes made during successive commits. Unfortunately, existing code differencing tools are not able to correctly identify changes made to workflow files. We therefore implemented gawd, a syntactic differencing tool for GitHub Actions workflows. The tool is capable of reporting the addition, deletion, modification and move of syntactic components in workflow files, taking into account the specific syntax of workflows. gawd has been evaluated on manually classified sets of workflow changes taken from existing commits in 40 different GitHub repositories, and was able to successfully identify these changes. gawd is publicly released as an open source Python tool distributed on PyPI.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644873,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555706,workflow automation;diff tool;software repository mining;GitHub;software changes;software evolution,Codes;Statistical analysis;Focusing;Syntactics;Software;Libraries;Encoding,,,,28.0,,18 Jun 2024,,,IEEE,IEEE Conferences
RABBIT: A tool for identifying bot accounts based on their recent GitHub event history,N. Chidambaram; T. Mens; A. Decan,"Software Engineering Lab University of Mons, Mons, Belgium; Software Engineering Lab University of Mons, Mons, Belgium; F.R.S.-FNRS Research Associate Software Engineering Lab University of Mons, Mons, Belgium",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,687,691,"Collaborative software development through GitHub repositories frequently relies on bot accounts to automate repetitive and error-prone tasks. This highlights the need to have accurate and efficient bot identification tools. Several such tools have been proposed in the past, but they tend to rely on a substantial amount of historical data, or they limit themselves to a reduced subset of activity types, making them difficult to use at large scale. To overcome these limitations, we developed RABBIT, an open source command-line tool that queries the GitHub Events API to retrieve the recent events of a given GitHub account and predicts whether the account is a human or a bot. RABBIT is based on an XGBoost classification model that relies on six features related to account activities and achieves high performance, with an AUC, F1 score, precision and recall of 0.92. Compared to the state-of-the-art in bot identification, RABBIT exhibits a similar performance in terms of precision, recall and F1 score, while being more than an order of magnitude faster and requiring considerably less data. This makes RABBIT usable on a large scale, capable of processing several thousand accounts per hour efficiently.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644877,Service Public de Wallonie; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555785,GitHub events;classification model;bot identification,Rabbits;Collaborative software;Predictive models;Chatbots;Software;Data models;History,,2.0,,15.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Quantifying Security Issues in Reusable JavaScript Actions in GitHub Workflows,H. O. Delicheh; A. Decan; T. Mens,"University of Mons, Mons, Belgium; University of Mons, Mons, Belgium; University of Mons, Mons, Belgium",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,692,703,"GitHub’s integrated automated workflow mechanism called GitHub Actions promotes the use of Actions as reusable building blocks in workflows. The majority of those Actions are developed in JavaScript and depend on packages distributed through the npm package manager. Those packages can suffer from security vulnerabilities, potentially affecting the Actions that rely on them. Using a dataset of 8,107 JavaScript Actions, we analysed to which extent dependencies on npm packages expose these Actions to vulnerabilities. We observed that JavaScript Actions tend to rely on dozens of npm packages, and that the vast majority of them depend on npm package releases with known vulnerabilities. Most of these vulnerabilities are caused by indirect dependencies, making it diﬃcult for Actions maintainers to analyse their exposure to security vulnerabilities. Moreover, indirect dependencies are more likely to suffer from vulnerabilities of higher severity. We also studied to which extent security weaknesses occur in the source code of JavaScript Actions. To do so, we used CodeQL to detect security weaknesses, revealing that more than 54% of the studied JavaScript Actions contain at least one security weakness, and a small subset of these weaknesses recur frequently in their code. This justifies the need for further studies and more advanced tool support for addressing security issues in the GitHub Actions ecosystem.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644899,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555637,GitHub Actions;security vulnerabilities;security weaknesses;npm;dependency network;CodeQL,Codes;Automation;Source coding;Ecosystems;Software;Security;Software measurement,,3.0,,43.0,,18 Jun 2024,,,IEEE,IEEE Conferences
What Can Self-Admitted Technical Debt Tell Us About Security? A Mixed-Methods Study,N. E. D. Ferreyra; M. Shahin; M. Zahedi; S. Quadri; R. Scandariato,"Hamburg University of Technology, Hamburg, Germany; RMIT University, Melbourne, Australia; The University of Melbourne, Melbourne, Australia; Hamburg University of Technology, Hamburg, Germany; Hamburg University of Technology, Hamburg, Germany",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,704,715,"Self-Admitted Technical Debt (SATD) encompasses a wide array of sub-optimal design and implementation choices reported in software artefacts (e.g., code comments and commit messages) by developers themselves. Such reports have been central to the study of software maintenance and evolution over the last decades. However, they can also be deemed as dreadful sources of information on potentially exploitable vulnerabilities and security flaws. Objective: This work investigates the security implications of SATD from a technical and developer-centred perspective. On the one hand, it analyses whether security pointers disclosed inside SATD sources can be used to characterise vulnerabilities in Open-Source Software (OSS) projects and repositories. On the other hand, it delves into developers’ perspectives regarding the motivations behind this practice, its prevalence, and its potential negative consequences. Method: We followed a mixed-methods approach consisting of (i) the analysis of a preexisting dataset containing 8,812 SATD instances and (ii) an online survey with 222 OSS practitioners. Results: We gathered 201 SATD instances through the dataset analysis and mapped them to different Common Weakness Enumeration (CWE) identifiers. Overall, 25 different types of CWEs were spotted across commit messages, pull requests, code comments, and issue sections, from which 8 appear among MITRE’s Top-25 most dangerous ones. The survey shows that software practitioners often place security pointers across SATD artefacts to promote a security culture among their peers and help them spot flaky code sections, among other motives. However, they also consider such a practice risky as it may facilitate vulnerability exploits. Implications: Our findings suggest that preserving the contextual integrity of security pointers disseminated across SATD artefacts is critical to safeguard both commercial and OSS solutions against zero-day attacks.CCS CONCEPTS•Security and privacy → Human and societal aspects of security and privacy; Software security engineering; • Software and its engineering → Maintaining software.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644909,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555782,self-admitted technical debt;software security;software engineering;technical debt identification,Surveys;Privacy;Software maintenance;Codes;Software;Security;Data mining,,1.0,,42.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Are Latent Vulnerabilities Hidden Gems for Software Vulnerability Prediction? An Empirical Study,T. H. Minh Le; X. Du; M. A. Babar,"CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide, Adelaide, Australia; Monash University, Melbourne, Victoria, Australia; CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide, Adelaide, Australia",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,716,727,"Collecting relevant and high-quality data is integral to the development of effective Software Vulnerability (SV) prediction models. Most of the current SV datasets rely on SV-fixing commits to extract vulnerable functions and lines. However, none of these datasets have considered latent SVs existing between the introduction and fix of the collected SVs. There is also little known about the usefulness of these latent SVs for SV prediction. To bridge these gaps, we conduct a large-scale study on the latent vulnerable functions in two commonly used SV datasets and their utilization for function-level and line-level SV predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more than 100k latent vulnerable functions in the studied datasets. We find that these latent functions can increase the number of SVs by 4× on average and correct up to 5k mislabeled functions, yet they have a noise level of around 6%. Despite the noise, we show that the state-of-the-art SV prediction model can significantly benefit from such latent SVs. The improvements are up to 24.5% in the performance (F1-Score) of function-level SV predictions and up to 67% in the effectiveness of localizing vulnerable lines. Overall, our study presents the first promising step toward the use of latent SVs to improve the quality of SV datasets and enhance the performance of SV prediction tasks.CCS CONCEPTS• Security and privacy → Software security engineering.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644919,Australian Government; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555654,Software vulnerability;Software security;Deep learning;Data quality;SZZ algorithm,Data privacy;Software algorithms;Noise;Predictive models;Prediction algorithms;Software;Data models,,2.0,,72.0,,18 Jun 2024,,,IEEE,IEEE Conferences
MalwareBench: Malware samples are not enough,N. Zahan; P. Burckhardt; M. Lysenko; F. Aboukhadijeh; L. Williams,"North Carolina State University, Raleigh, NC, USA; Socket, Inc, Wilmington, DE, USA; Socket, Inc, Wilmington, DE, USA; Socket, Inc, Wilmington, DE, USA; North Carolina State University, Raleigh, NC, USA",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,728,732,"The prevalent use of third-party components in modern software development, rapid modernization, and digitization have significantly amplified the risk of software supply chain attacks. Popular large registries like npm and PyPI are highly targeted malware distribution channels for attackers due to heavy growth and dependence on third-party components. Industry and academia are working towards building tools to detect malware in the software supply chain. However, a lack of benchmark datasets containing both malicious and neutral packages hampers the evaluation of the performance of these malware detection tools. The goal of our study is to aid researchers and tool developers in evaluating and improving malware detection tools by contributing a benchmark dataset built by systematically collecting malicious and neutral packages from the npm and PyPI ecosystems. We present MalwareBench, a labeled dataset of 20,792 packages (of which 6,659 are malicious) from the npm and PyPI ecosystems. We constructed the benchmark dataset by incorporating pre-existing malware datasets with the Socket internal benchmark data and including popular and newly released npm and PyPI packages. The ground truth labels of these neutral packages were determined using the Socket AI Scanner and manual inspection.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644883,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555755,Software Engineering Security;Software Supply Chain;Software Supply Chain Security;npm and PyPI Ecosystems;Malicious Packages;Benchmark Dataset,Industries;Sockets;Supply chains;Ecosystems;Manuals;Benchmark testing;Inspection,,1.0,,23.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Hash4Patch: A Lightweight Low False Positive Tool for Finding Vulnerability Patch Commits,S. Scalco; R. Paramitha,"Università Degli Studi di Trento, Trento, Italy; Università Degli Studi di Trento, Trento, Italy",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,733,737,"[Context:] Patch commits are useful to complete vulnerability datasets for training ML models and for developers to find a safe version for their dependencies. [Objective:] However, there is a gap in the state-of-the-art (SOTA) for a lightweight low False Positive patch commit finder. [Method:] We implemented Hash4Patch, a new tool to be used along with a current SOTA patch finder. We then validated it with a dataset of 160 CVEs. [Results:] Our approach significantly reduced the False Positives produced by a state-of-the-art tool with only 1 minute of additional running time on average. [Conclusions:] Our tool is able to effectively and eﬃciently reduce the number of alerts found by other patch commit finders, thus minimizing the manual effort needed by developers.CCS CONCEPTS•Software and its engineering → Software configuration management and version control systems; • Security and privacy → Software security engineering.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644871,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555742,Vulnerability;Patch commit;Lightweight;Hash search,Training;Data privacy;Configuration management;Companies;Control systems;Software;Security,,,,21.0,,18 Jun 2024,,,IEEE,IEEE Conferences
MegaVul: A C/C++ Vulnerability Dataset with Comprehensive Code Representations,C. Ni; L. Shen; X. Yang; Y. Zhu; S. Wang,"Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Central University of Finance and Economics, Beijing, China",2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,738,742,"We constructed a newly large-scale and comprehensive C/C++ vulnerability dataset named MegaVul by crawling the Common Vulnerabilities and Exposures (CVE) database and CVE-related open-source projects. Specifically, we collected all crawlable descriptive information of the vulnerabilities from the CVE database and extracted all vulnerability-related code changes from 28 Git-based websites. We adopt advanced tools to ensure the extracted code integrality and enrich the code with four different transformed representations. Totally, MegaVul contains 17,380 vulnerabilities collected from 992 open-source repositories spanning 169 different vulnerability types disclosed from January 2006 to October 2023. Thus, MegaVul can be used for a variety of software security-related tasks including detecting vulnerabilities and assessing vulnerability severity. All information is stored in the JSON format for easy usage. MegaVul is publicly available on GitHub and will be continuously updated. It can be easily extended to other programming languages.CCS CONCEPTS•Software and its engineering → Software defect analysis.",2574-3864,979-8-4007-0587-8,10.1145/3643991.3644886,National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555623,Common Vulnerabilities and Exposures;C/C++ Code;Code Representation,Codes;Filters;Databases;Web services;Syntactics;Software;Data mining,,2.0,,20.0,,18 Jun 2024,,,IEEE,IEEE Conferences
Author Index,,,2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR),18 Jun 2024,2024,,,743,747,,2574-3864,979-8-4007-0587-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555808,,,,,,,,18 Jun 2024,,,IEEE,IEEE Conferences
